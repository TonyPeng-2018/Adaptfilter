{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 226.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10 mobilenet\n",
      "0.8240985870361328, 1.9571852684020996, 2.882969379425049, 3.5321617126464844, 4.309632778167725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this file is for device on the client side\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "# This file is for trainning\n",
    "# Run this on the server, or as we called offline. \n",
    "\n",
    "import argparse\n",
    "import base64\n",
    "import cv2\n",
    "import datetime\n",
    "from Models import gatedmodel,mobilenetv2, resnet\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import psutil\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from Utils import utils, encoder\n",
    "\n",
    "middle_sizes_mobile = [1,2,4,8,16]\n",
    "middle_sizes_resnet = [1,2,4,8,16,32]\n",
    "\n",
    "middle_sizes = {'mobilenet': middle_sizes_mobile, 'resnet': middle_sizes_resnet}\n",
    "reduced_sizes = {'cifar-10': (32,32), 'imagenet': (224,224)}\n",
    "reduced_rates = {'mobilenet': 2, 'resnet': 4}\n",
    "\n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "i_stop = 100\n",
    "\n",
    "width, height = reduced_sizes[dataset][0]/reduced_rates[model], \\\n",
    "                reduced_sizes[dataset][1]/reduced_rates[model]\n",
    "middle_size = middle_sizes[model]\n",
    "\n",
    "# client include client, middle and gate\n",
    "\n",
    "client = mobilenetv2.mobilenetv2_splitter_client(num_classes = 10, weight_root='./Weights/'+dataset+'/', device='cpu')\n",
    "# client = resnet.resnet_splitter_client(num_classes=1000, weight_root='./Weights/'+dataset+'/', device='cpu', layers=50)\n",
    "\n",
    "middle_models = []\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models.append(mobilenetv2.MobileNetV2_middle(middle=middle_size[i]))\n",
    "    # middle_models.append(resnet.resnet_middle(middle=middle_size[i]))\n",
    "\n",
    "gate_models = []\n",
    "for i in range(len(middle_size)):\n",
    "    gate_models.append(gatedmodel.ExitGate(in_planes=middle_size[i],\n",
    "                                           height = height, width=width))\n",
    "\n",
    "# eval\n",
    "client.eval()\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models[i].eval()\n",
    "    gate_models[i].eval()\n",
    "\n",
    "# quantize\n",
    "client = torch.ao.quantization.quantize_dynamic(client, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models[i] = torch.ao.quantization.quantize_dynamic(middle_models[i], {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "    gate_models[i] = torch.ao.quantization.quantize_dynamic(gate_models[i], {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "# 2. dataset\n",
    "# directly read bmp image from the storage\n",
    "data_root = '../data/'+dataset+'-client/'\n",
    "data_client_out = []\n",
    "for i in range(len(middle_size)):\n",
    "    data_client_out.append('../data/'+dataset+'-'+model+'-client-'+str(middle_size[i])+'/')\n",
    "    if not os.path.exists(data_client_out[i]):\n",
    "        os.makedirs(data_client_out[i])\n",
    "    \n",
    "images_list = os.listdir(data_root)\n",
    "images_list.remove('labels.txt')\n",
    "# remove ending with jpg\n",
    "images_list = [x for x in images_list if x.endswith('.bmp')]\n",
    "images_list = sorted(images_list)\n",
    "\n",
    "client_time = [0] * len(middle_size)\n",
    "\n",
    "# this is test the overspeed, so we don't need to load the models\n",
    "with torch.no_grad():\n",
    "        for i, i_path in tqdm(enumerate(images_list)):\n",
    "            if i >= i_stop:\n",
    "                break\n",
    "            \n",
    "            image_path = data_root + i_path\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            image = torch.tensor(image)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.permute(0, 3, 1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                s_time = time.time()\n",
    "                client_out = client(image).detach()\n",
    "                for j in range(len(middle_size)):\n",
    "                    middle_in = middle_models[j].in_layer(client_out)\n",
    "                    gate_out = gate_models[j](middle_in)\n",
    "                    middle_in = middle_in.squeeze(0)\n",
    "                    middle_int = utils.float_to_uint(middle_in)\n",
    "                    middle_int = middle_int.numpy().copy(order='C')\n",
    "                    middle_int = middle_int.astype(np.uint8)\n",
    "                    send_in = base64.b64encode(middle_int)\n",
    "                    # store it in the folder\n",
    "                    with open(data_client_out[j] + i_path[:-4], 'wb') as f:\n",
    "                        f.write(send_in)\n",
    "                    s1_time = time.time()\n",
    "                    client_time[j] += s1_time - s_time\n",
    "\n",
    "for i in range(len(client_time)):\n",
    "    client_time[i] /= i_stop / 1000 # ms per frame\n",
    "\n",
    "# print the list without [ and ]\n",
    "out_string = str(client_time).replace('[','').replace(']','')\n",
    "print(dataset, model)\n",
    "print(out_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a image and store it\n",
    "# image = cv2.imread('../data/imagenet-client/0.bmp', cv2.IMREAD_COLOR)\n",
    "# # encode\n",
    "# image = image.astype(np.uint8)\n",
    "# image = base64.b64encode(image)\n",
    "# # store\n",
    "# with open('../data/imagenet-client/0b', 'wb') as f:\n",
    "#     f.write(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "data_root = '../data/imagenet-client/'\n",
    "# get '.bmp'\n",
    "import os \n",
    "images_list = os.listdir(data_root)\n",
    "images_list = [x for x in images_list if x.endswith('.bmp')]\n",
    "\n",
    "data_out = '../data/imagenet-jpeg/'\n",
    "\n",
    "import cv2\n",
    "# compress the image jpeg\n",
    "for i_path in images_list:\n",
    "    image_path = data_root + i_path\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    for quality in range(5, 100, 5):\n",
    "        cv2.imwrite(data_out+i_path[:-4]+'_'+str(quality)+'.jpg', image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cifar-10-jpeg25/\n",
      "[0.65, 0.7, 0.6333333333333333, 0.6833333333333333, 0.65, 0.7, 0.7166666666666667, 0.6666666666666666, 0.7666666666666667, 0.7166666666666667]\n",
      "[3.6353707313537598, 3.5940567652384443, 3.534579277038574, 3.5523414611816406, 3.596754868825277, 3.6237597465515137, 3.4986217816670733, 3.618709246317546, 3.582139809926351, 3.4907301266988116]\n"
     ]
    }
   ],
   "source": [
    "# load embbeded \n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "model = mobilenetv2.MobileNetV2(num_classes=10)\n",
    "model.load_state_dict(torch.load('./Weights/'+dataset+'/pretrained/mobilenetv2.pth'))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "jpeg25_path = '../data/'+dataset+'-jpeg25/'\n",
    "root_path = '../data/'+dataset+'-client/'\n",
    "labels = root_path+'labels.txt'\n",
    "labels = open(labels, 'r')\n",
    "labels = labels.readlines()\n",
    "labels = [x[:-1] for x in labels]\n",
    "labels = torch.tensor([int(x) for x in labels])\n",
    "labels\n",
    "\n",
    "images = [str(x) for x in range(600)]\n",
    "preds = []\n",
    "\n",
    "from torchvision import transforms\n",
    "if dataset == 'cifar-10':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "elif dataset == 'imagenet':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "with torch.no_grad():\n",
    "    server_time = []\n",
    "    for image in images:\n",
    "        \n",
    "        image_path = jpeg25_path + image\n",
    "        i_read = open(image_path, 'rb')\n",
    "        image = i_read.read()\n",
    "        # decode\n",
    "        s_time = time.time()\n",
    "        image = base64.b64decode(image)\n",
    "        image = np.frombuffer(image, dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        image = normal(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to('cuda')\n",
    "        pred = model(image)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.append(pred.item())\n",
    "        e_time = time.time()\n",
    "        server_time.append(e_time - s_time)\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "accuracy = [0] * 10 \n",
    "server_time_box = [0] * 10\n",
    "# calculate the accuracy every 60 frames\n",
    "for i in range(0, 600, 60):\n",
    "    accuracy[i//60] = torch.sum(preds[i:i+60] == labels[i:i+60]).item() / 60\n",
    "    server_time_box[i//60] = sum(server_time[i:i+60]) / 60*1000\n",
    "\n",
    "print(jpeg25_path)\n",
    "print(accuracy)\n",
    "print(server_time_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cifar-10-jpeg75/\n",
      "[0.8, 0.8666666666666667, 0.7666666666666667, 0.7833333333333333, 0.7166666666666667, 0.7666666666666667, 0.8333333333333334, 0.7666666666666667, 0.9, 0.8833333333333333]\n",
      "[3.3351898193359375, 3.51340373357137, 3.528730074564616, 3.3399899800618487, 3.288976351420085, 3.2774964968363447, 3.330787022908529, 3.3434510231018066, 3.3651749293009443, 3.3732573191324873]\n"
     ]
    }
   ],
   "source": [
    "# load embbeded \n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "model = mobilenetv2.MobileNetV2(num_classes=10)\n",
    "model.load_state_dict(torch.load('./Weights/'+dataset+'/pretrained/mobilenetv2.pth'))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "jpeg75_path = '../data/'+dataset+'-jpeg75/'\n",
    "root_path = '../data/'+dataset+'-client/'\n",
    "labels = root_path+'labels.txt'\n",
    "labels = open(labels, 'r')\n",
    "labels = labels.readlines()\n",
    "labels = [x[:-1] for x in labels]\n",
    "labels = torch.tensor([int(x) for x in labels])\n",
    "labels\n",
    "\n",
    "images = [str(x) for x in range(600)]\n",
    "preds = []\n",
    "\n",
    "from torchvision import transforms\n",
    "if dataset == 'cifar-10':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "elif dataset == 'imagenet':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "with torch.no_grad():\n",
    "    server_time = []\n",
    "    for image in images:\n",
    "        \n",
    "        image_path = jpeg75_path + image\n",
    "        i_read = open(image_path, 'rb')\n",
    "        image = i_read.read()\n",
    "        # decode\n",
    "        s_time = time.time()\n",
    "        image = base64.b64decode(image)\n",
    "        image = np.frombuffer(image, dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        image = normal(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to('cuda')\n",
    "        pred = model(image)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.append(pred.item())\n",
    "        e_time = time.time()\n",
    "        server_time.append(e_time - s_time)\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "accuracy = [0] * 10 \n",
    "server_time_box = [0] * 10\n",
    "# calculate the accuracy every 60 frames\n",
    "for i in range(0, 600, 60):\n",
    "    accuracy[i//60] = torch.sum(preds[i:i+60] == labels[i:i+60]).item() / 60\n",
    "    server_time_box[i//60] = sum(server_time[i:i+60]) / 60*1000\n",
    "\n",
    "print(jpeg75_path)\n",
    "print(accuracy)\n",
    "print(server_time_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cifar-10-cjpeg/\n",
      "[0.8166666666666667, 0.8, 0.7666666666666667, 0.8166666666666667, 0.7166666666666667, 0.8, 0.8333333333333334, 0.7833333333333333, 0.8666666666666667, 0.8666666666666667]\n",
      "[3.3003171284993487, 3.4371654192606607, 3.4213980038960776, 3.306754430135091, 3.404005368550618, 3.368230660756429, 3.4621795018513994, 3.341809908548991, 3.330520788828532, 3.3794204394022627]\n"
     ]
    }
   ],
   "source": [
    "# load embbeded \n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "model = mobilenetv2.MobileNetV2(num_classes=10)\n",
    "model.load_state_dict(torch.load('./Weights/'+dataset+'/pretrained/mobilenetv2.pth'))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "cjpeg_path = '../data/'+dataset+'-cjpeg/'\n",
    "root_path = '../data/'+dataset+'-client/'\n",
    "labels = root_path+'labels.txt'\n",
    "labels = open(labels, 'r')\n",
    "labels = labels.readlines()\n",
    "labels = [x[:-1] for x in labels]\n",
    "labels = torch.tensor([int(x) for x in labels])\n",
    "labels\n",
    "\n",
    "images = [str(x) for x in range(600)]\n",
    "preds = []\n",
    "\n",
    "from torchvision import transforms\n",
    "if dataset == 'cifar-10':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "elif dataset == 'imagenet':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "with torch.no_grad():\n",
    "    server_time = []\n",
    "    for image in images:\n",
    "        \n",
    "        image_path = cjpeg_path + image\n",
    "        i_read = open(image_path, 'rb')\n",
    "        image = i_read.read()\n",
    "        # decode\n",
    "        s_time = time.time()\n",
    "        image = base64.b64decode(image)\n",
    "        image = np.frombuffer(image, dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        image = normal(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to('cuda')\n",
    "        pred = model(image)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.append(pred.item())\n",
    "        e_time = time.time()\n",
    "        server_time.append(e_time - s_time)\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "accuracy = [0] * 10 \n",
    "server_time_box = [0] * 10\n",
    "# calculate the accuracy every 60 frames\n",
    "for i in range(0, 600, 60):\n",
    "    accuracy[i//60] = torch.sum(preds[i:i+60] == labels[i:i+60]).item() / 60\n",
    "    server_time_box[i//60] = sum(server_time[i:i+60]) / 60*1000\n",
    "\n",
    "print(cjpeg_path)\n",
    "print(accuracy)\n",
    "print(server_time_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173 113  53 ...  15  69 116]\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimdecode(image, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(image)\n\u001b[0;32m---> 53\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torchvision/transforms/functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# load embbeded \n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "model = mobilenetv2.MobileNetV2(num_classes=10)\n",
    "model.load_state_dict(torch.load('./Weights/'+dataset+'/pretrained/mobilenetv2.pth'))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "cjpeg_path = '../data/'+dataset+'-cjpeg-ML/'\n",
    "root_path = '../data/'+dataset+'-client/'\n",
    "labels = root_path+'labels.txt'\n",
    "labels = open(labels, 'r')\n",
    "labels = labels.readlines()\n",
    "labels = [x[:-1] for x in labels]\n",
    "labels = torch.tensor([int(x) for x in labels])\n",
    "labels\n",
    "\n",
    "images = [str(x) for x in range(600)]\n",
    "preds = []\n",
    "\n",
    "from torchvision import transforms\n",
    "if dataset == 'cifar-10':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "elif dataset == 'imagenet':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "with torch.no_grad():\n",
    "    server_time = []\n",
    "    for image in images:\n",
    "        \n",
    "        image_path = cjpeg_path + image\n",
    "        # check is the path exist\n",
    "        i_read = open(image_path, 'rb')\n",
    "        image = i_read.read()\n",
    "        \n",
    "        # decode\n",
    "        s_time = time.time()\n",
    "        image = base64.b64decode(image)\n",
    "        image = np.frombuffer(image, dtype=np.uint8)\n",
    "        print(len(image))\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        print(image)\n",
    "        image = normal(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to('cuda')\n",
    "        pred = model(image)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.append(pred.item())\n",
    "        e_time = time.time()\n",
    "        server_time.append(e_time - s_time)\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "accuracy = [0] * 10 \n",
    "server_time_box = [0] * 10\n",
    "# calculate the accuracy every 60 frames\n",
    "for i in range(0, 600, 60):\n",
    "    accuracy[i//60] = torch.sum(preds[i:i+60] == labels[i:i+60]).item() / 60\n",
    "    server_time_box[i//60] = sum(server_time[i:i+60]) / 60*1000\n",
    "\n",
    "print(cjpeg_path)\n",
    "print(accuracy)\n",
    "print(server_time_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8666666666666667, 0.85, 0.7833333333333333, 0.8166666666666667, 0.7333333333333333, 0.8333333333333334, 0.8666666666666667, 0.8, 0.8833333333333333, 0.8666666666666667]\n"
     ]
    }
   ],
   "source": [
    "# load embbeded \n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "model = mobilenetv2.MobileNetV2(num_classes=10)\n",
    "model.load_state_dict(torch.load('./Weights/'+dataset+'/pretrained/mobilenetv2.pth'))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "original_path = '../data/'+dataset+'-client/'\n",
    "root_path = '../data/'+dataset+'-client/'\n",
    "labels = root_path+'labels.txt'\n",
    "labels = open(labels, 'r')\n",
    "labels = labels.readlines()\n",
    "labels = [x[:-1] for x in labels]\n",
    "labels = torch.tensor([int(x) for x in labels])\n",
    "\n",
    "images = [str(x)+'.bmp' for x in range(600)]\n",
    "preds = []\n",
    "\n",
    "from torchvision import transforms\n",
    "if dataset == 'cifar-10':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "elif dataset == 'imagenet':\n",
    "    normal = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "from PIL import Image\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = original_path + image\n",
    "        i_read = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(i_read)\n",
    "        image = image.astype(np.float32)/255.0\n",
    "        # print(image)\n",
    "        image = normal(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to('cuda')\n",
    "        \n",
    "        pred = model(image)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        preds.append(pred.item())\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "accuracy = [0] * 10 \n",
    "# calculate the accuracy every 60 frames\n",
    "for i in range(0, 600, 60):\n",
    "    accuracy[i//60] = torch.sum(preds[i:i+60] == labels[i:i+60]).item() / 60\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "tensor([[-6.8665, -2.0951, -7.5659,  2.3243, -5.8989,  0.6056, -3.0840, -3.9598,\n",
      "         -4.0189, -4.1408]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([[ -1.7792,   0.8832, -11.1968, -10.5257, -11.8882, -11.5903, -13.3459,\n",
      "         -13.7443,   5.9686,  -4.3372]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([[  0.8940,  -2.6838,  -5.0191,  -7.0503,  -5.6024,  -7.5521, -11.0815,\n",
      "          -6.9351,   4.1427,  -4.1215]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([[ 3.8037, -2.8443, -2.3276, -5.0625, -8.1910, -7.3822, -6.0312, -7.6261,\n",
      "         -2.6992, -3.6173]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ -9.6393,  -8.6659,  -4.3395,  -2.3219,  -1.1398,  -8.2178,   5.5698,\n",
      "          -7.3277, -12.7354, -11.7017]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([[ -8.3568,  -7.8755,  -3.7223,  -0.2882,  -4.6380,  -1.9769,   4.7459,\n",
      "          -7.0902, -13.3183,  -8.0522]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([[ -6.2769,   1.1829,  -5.1391,  -0.8067, -10.4371,  -1.1274,  -0.6126,\n",
      "          -2.9121,  -8.1587,  -3.5914]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([[ -5.7293,  -4.6394,  -1.7989,  -0.3276,  -2.0962,  -3.4673,   2.8211,\n",
      "          -1.1878, -10.0510,  -5.3110]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([[ -9.6919, -14.4494,  -2.8463,   3.6878,   0.7572,  -0.0363,  -4.7574,\n",
      "          -5.4670,  -9.3448,  -8.1471]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([[ -7.0302,   4.3022,  -7.6952,  -8.5979, -10.9997,  -4.4761,  -5.9529,\n",
      "         -10.1695,  -5.9346,  -0.2354]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([[ 3.3783, -8.8224,  0.3874, -3.2876, -2.7256, -4.1143, -5.9198, -5.4739,\n",
      "         -4.8866, -6.7447]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from Dataloaders import dataloader_cifar10\n",
    "_, test, _ = dataloader_cifar10.Dataloader_cifar10_val(test_batch=1)\n",
    "for i, (image, label) in enumerate(test):\n",
    "    image = image.to('cuda')\n",
    "    pred = model(image)\n",
    "    pred = torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
