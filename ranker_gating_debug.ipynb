{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# first, load the embeddings\n",
    "# stored in torch tensors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats \n",
    "\n",
    "embeddings_folder = '../data/cifar-10-embedding-3/embeddings/' \n",
    "embeddings_files = sorted(os.listdir(embeddings_folder))\n",
    "test_embeddings = torch.load(embeddings_folder + embeddings_files[0]) # load the first file\n",
    "print(test_embeddings.size()) # 128, 32, 32, 32\n",
    "\n",
    "# calculate the entropy of the embeddings, select 1%, 5%, 10%, 20%, 50% of the embeddings\n",
    "def calculate_entropy(embs, percentage):\n",
    "    # embs are torch tensors\n",
    "    # percentage shows the number of embeddings to select\n",
    "    embs_entropy = []\n",
    "    for i in range(embs.size(len(embs.size())-3)):\n",
    "        embs_entropy.append(scipy.stats.entropy(embs[:, i].reshape(-1))) # b, c\n",
    "    # get the top percentage of the channels\n",
    "    embs_entropy = np.array(embs_entropy)\n",
    "    indices = np.argsort(embs_entropy)\n",
    "    num_selected = int(embs.size(1) * percentage)\n",
    "    selected_indices = indices[:num_selected]\n",
    "    return selected_indices\n",
    "    # out b, c*p, h, w or c*p, h, w\n",
    "\n",
    "def ranker_entropy(embs, percentage):\n",
    "    # calculate the entropy of the embeddings\n",
    "    selected_indices = calculate_entropy(embs, percentage)\n",
    "    # select indice from test embeddings\n",
    "    selected_embeddings = embs[:, selected_indices] # b, c*p, h, w\n",
    "    return selected_embeddings\n",
    "\n",
    "# get the selected embeddings\n",
    "selected_embeddings = ranker_entropy(test_embeddings, 0.1)\n",
    "print(selected_embeddings.size())\n",
    "\n",
    "# design a simple gating mechanism for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/391], Loss: 2.3065\n",
      "Epoch [1/10], Step [2/391], Loss: 2.2890\n",
      "Epoch [1/10], Step [3/391], Loss: 2.2958\n",
      "Epoch [1/10], Step [4/391], Loss: 2.3008\n",
      "Epoch [1/10], Step [5/391], Loss: 2.2991\n",
      "Epoch [1/10], Step [6/391], Loss: 2.2940\n",
      "Epoch [1/10], Step [7/391], Loss: 2.2865\n",
      "Epoch [1/10], Step [8/391], Loss: 2.2817\n",
      "Epoch [1/10], Step [9/391], Loss: 2.2805\n",
      "Epoch [1/10], Step [10/391], Loss: 2.2753\n",
      "Epoch [1/10], Step [11/391], Loss: 2.2757\n",
      "Epoch [1/10], Step [12/391], Loss: 2.2734\n",
      "Epoch [1/10], Step [13/391], Loss: 2.2697\n",
      "Epoch [1/10], Step [14/391], Loss: 2.2678\n",
      "Epoch [1/10], Step [15/391], Loss: 2.2661\n",
      "Epoch [1/10], Step [16/391], Loss: 2.2644\n",
      "Epoch [1/10], Step [17/391], Loss: 2.2625\n",
      "Epoch [1/10], Step [18/391], Loss: 2.2644\n",
      "Epoch [1/10], Step [19/391], Loss: 2.2622\n",
      "Epoch [1/10], Step [20/391], Loss: 2.2608\n",
      "Epoch [1/10], Step [21/391], Loss: 2.2590\n",
      "Epoch [1/10], Step [22/391], Loss: 2.2609\n",
      "Epoch [1/10], Step [23/391], Loss: 2.2597\n",
      "Epoch [1/10], Step [24/391], Loss: 2.2579\n",
      "Epoch [1/10], Step [25/391], Loss: 2.2564\n",
      "Epoch [1/10], Step [26/391], Loss: 2.2541\n",
      "Epoch [1/10], Step [27/391], Loss: 2.2515\n",
      "Epoch [1/10], Step [28/391], Loss: 2.2509\n",
      "Epoch [1/10], Step [29/391], Loss: 2.2484\n",
      "Epoch [1/10], Step [30/391], Loss: 2.2466\n",
      "Epoch [1/10], Step [31/391], Loss: 2.2446\n",
      "Epoch [1/10], Step [32/391], Loss: 2.2434\n",
      "Epoch [1/10], Step [33/391], Loss: 2.3008\n",
      "Epoch [1/10], Step [34/391], Loss: 2.2977\n",
      "Epoch [1/10], Step [35/391], Loss: 2.2935\n",
      "Epoch [1/10], Step [36/391], Loss: 2.2890\n",
      "Epoch [1/10], Step [37/391], Loss: 2.2833\n",
      "Epoch [1/10], Step [38/391], Loss: 2.2783\n",
      "Epoch [1/10], Step [39/391], Loss: 2.2744\n",
      "Epoch [1/10], Step [40/391], Loss: 2.2708\n",
      "Epoch [1/10], Step [41/391], Loss: 2.2691\n",
      "Epoch [1/10], Step [42/391], Loss: 2.2668\n",
      "Epoch [1/10], Step [43/391], Loss: 2.2646\n",
      "Epoch [1/10], Step [44/391], Loss: 2.2627\n",
      "Epoch [1/10], Step [45/391], Loss: 2.2605\n",
      "Epoch [1/10], Step [46/391], Loss: 2.2582\n",
      "Epoch [1/10], Step [47/391], Loss: 2.2553\n",
      "Epoch [1/10], Step [48/391], Loss: 2.2524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m selected_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mranker_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     96\u001b[0m selected_embeddings \u001b[38;5;241m=\u001b[39m selected_embeddings\u001b[38;5;241m.\u001b[39mcuda()\n",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36mranker_entropy\u001b[0;34m(embs, percentage)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mranker_entropy\u001b[39m(embs, percentage):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# calculate the entropy of the embeddings\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     selected_indices \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# select indice from test embeddings\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     selected_embeddings \u001b[38;5;241m=\u001b[39m embs[:, selected_indices] \u001b[38;5;66;03m# b, c*p, h, w\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mcalculate_entropy\u001b[0;34m(embs, percentage)\u001b[0m\n\u001b[1;32m     26\u001b[0m embs_entropy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(embs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mlen\u001b[39m(embs\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)):\n\u001b[0;32m---> 28\u001b[0m     embs_entropy\u001b[38;5;241m.\u001b[39mappend(\u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# b, c\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the top percentage of the channels\u001b[39;00m\n\u001b[1;32m     30\u001b[0m embs_entropy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embs_entropy)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentinel:\n\u001b[1;32m    530\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _remove_sentinel(samples, paired, sentinel)\n\u001b[0;32m--> 531\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    533\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/scipy/stats/_entropy.py:144\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(pk, qk, base, axis)\u001b[0m\n\u001b[1;32m    142\u001b[0m pk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pk)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     pk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39mpk \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     vec \u001b[38;5;241m=\u001b[39m special\u001b[38;5;241m.\u001b[39mentr(pk)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats\n",
    "import torch.utils \n",
    "import os\n",
    "\n",
    "# import the server model\n",
    "\n",
    "class GatedRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GatedRegression, self).__init__()\n",
    "        # think about it 3*32*32 -> 1\n",
    "        # think about the classification of the mobile net\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = input_size\n",
    "        self.conv1 = nn.Conv2d(input_size, 2*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(2*self.hidden_size)\n",
    "        self.conv2 = nn.Conv2d(2*self.hidden_size, 4*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*self.hidden_size)\n",
    "        self.conv3 = nn.Conv2d(4*self.hidden_size, 8*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(8*self.hidden_size)\n",
    "        self.linear = nn.Linear(8*self.hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(4)\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # need to change 32 -> 4\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "# load the dataset\n",
    "class gated_dataset(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, embeddings_folder):\n",
    "        self.embeddings_folder = embeddings_folder\n",
    "        self.embeddings_files = sorted(os.listdir(embeddings_folder+'/embeddings/'))\n",
    "        self.labels_files = sorted(os.listdir(embeddings_folder+'/labels/'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings_files.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            self.embeddings = torch.load(self.embeddings_folder+'/embeddings/' + self.embeddings_files[idx])\n",
    "            self.labels = torch.load(self.embeddings_folder+'/labels/' + self.labels_files[idx])\n",
    "        else:\n",
    "            self.embeddings = torch.cat((self.embeddings, torch.load(embeddings_folder+'/embeddings/' + embeddings_files[idx])), 0)\n",
    "            self.labels = torch.cat((self.labels, torch.load(embeddings_folder+'/labels/' + self.labels_files[idx])), 0)\n",
    "        return self.embeddings, self.labels\n",
    "    \n",
    "# load the dataset\n",
    "embeddings_folder = '../data/cifar-10-embedding-3/'\n",
    "dataset = gated_dataset(embeddings_folder)\n",
    "# print(gated_dataset.__len__(dataset)) # 391 batches                                             \n",
    "\n",
    "# load the data loader, train no test\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# load the model\n",
    "Gated = GatedRegression(input_size=3, num_classes=10)\n",
    "Gated = Gated.cuda()\n",
    "\n",
    "# load the optimizer\n",
    "optimizer = optim.Adam(Gated.parameters(), lr=0.001)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        embeddings, labels = data\n",
    "        # embeddings = Variable(embeddings) # what are they?\n",
    "        # labels = Variable(labels) # what are they?\n",
    "        embeddings = embeddings.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        selected_embeddings = ranker_entropy(embeddings, 0.1)\n",
    "        optimizer.zero_grad()\n",
    "        selected_embeddings = selected_embeddings.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = Gated(selected_embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch+1, 10, i+1, len(train_loader), loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
