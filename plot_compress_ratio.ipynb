{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 226.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10 mobilenet\n",
      "0.8240985870361328, 1.9571852684020996, 2.882969379425049, 3.5321617126464844, 4.309632778167725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this file is for device on the client side\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "# This file is for trainning\n",
    "# Run this on the server, or as we called offline. \n",
    "\n",
    "import argparse\n",
    "import base64\n",
    "import cv2\n",
    "import datetime\n",
    "from Models import gatedmodel,mobilenetv2, resnet\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import psutil\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from Utils import utils, encoder\n",
    "\n",
    "middle_sizes_mobile = [1,2,4,8,16]\n",
    "middle_sizes_resnet = [1,2,4,8,16,32]\n",
    "\n",
    "middle_sizes = {'mobilenet': middle_sizes_mobile, 'resnet': middle_sizes_resnet}\n",
    "reduced_sizes = {'cifar-10': (32,32), 'imagenet': (224,224)}\n",
    "reduced_rates = {'mobilenet': 2, 'resnet': 4}\n",
    "\n",
    "dataset = 'cifar-10'\n",
    "model = 'mobilenet'\n",
    "i_stop = 100\n",
    "\n",
    "width, height = reduced_sizes[dataset][0]/reduced_rates[model], \\\n",
    "                reduced_sizes[dataset][1]/reduced_rates[model]\n",
    "middle_size = middle_sizes[model]\n",
    "\n",
    "# client include client, middle and gate\n",
    "\n",
    "client = mobilenetv2.mobilenetv2_splitter_client(num_classes = 10, weight_root='./Weights/'+dataset+'/', device='cpu')\n",
    "# client = resnet.resnet_splitter_client(num_classes=1000, weight_root='./Weights/'+dataset+'/', device='cpu', layers=50)\n",
    "\n",
    "middle_models = []\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models.append(mobilenetv2.MobileNetV2_middle(middle=middle_size[i]))\n",
    "    # middle_models.append(resnet.resnet_middle(middle=middle_size[i]))\n",
    "\n",
    "gate_models = []\n",
    "for i in range(len(middle_size)):\n",
    "    gate_models.append(gatedmodel.ExitGate(in_planes=middle_size[i],\n",
    "                                           height = height, width=width))\n",
    "\n",
    "# eval\n",
    "client.eval()\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models[i].eval()\n",
    "    gate_models[i].eval()\n",
    "\n",
    "# quantize\n",
    "client = torch.ao.quantization.quantize_dynamic(client, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "for i in range(len(middle_size)):\n",
    "    middle_models[i] = torch.ao.quantization.quantize_dynamic(middle_models[i], {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "    gate_models[i] = torch.ao.quantization.quantize_dynamic(gate_models[i], {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "# 2. dataset\n",
    "# directly read bmp image from the storage\n",
    "data_root = '../data/'+dataset+'-client/'\n",
    "data_client_out = []\n",
    "for i in range(len(middle_size)):\n",
    "    data_client_out.append('../data/'+dataset+'-'+model+'-client-'+str(middle_size[i])+'/')\n",
    "    if not os.path.exists(data_client_out[i]):\n",
    "        os.makedirs(data_client_out[i])\n",
    "    \n",
    "images_list = os.listdir(data_root)\n",
    "images_list.remove('labels.txt')\n",
    "# remove ending with jpg\n",
    "images_list = [x for x in images_list if x.endswith('.bmp')]\n",
    "images_list = sorted(images_list)\n",
    "\n",
    "client_time = [0] * len(middle_size)\n",
    "\n",
    "# this is test the overspeed, so we don't need to load the models\n",
    "with torch.no_grad():\n",
    "        for i, i_path in tqdm(enumerate(images_list)):\n",
    "            if i >= i_stop:\n",
    "                break\n",
    "            \n",
    "            image_path = data_root + i_path\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            image = image.astype(np.float32)/255.0\n",
    "            image = torch.tensor(image)\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.permute(0, 3, 1, 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                s_time = time.time()\n",
    "                client_out = client(image).detach()\n",
    "                for j in range(len(middle_size)):\n",
    "                    middle_in = middle_models[j].in_layer(client_out)\n",
    "                    gate_out = gate_models[j](middle_in)\n",
    "                    middle_in = middle_in.squeeze(0)\n",
    "                    middle_int = utils.float_to_uint(middle_in)\n",
    "                    middle_int = middle_int.numpy().copy(order='C')\n",
    "                    middle_int = middle_int.astype(np.uint8)\n",
    "                    send_in = base64.b64encode(middle_int)\n",
    "                    # store it in the folder\n",
    "                    with open(data_client_out[j] + i_path[:-4], 'wb') as f:\n",
    "                        f.write(send_in)\n",
    "                    s1_time = time.time()\n",
    "                    client_time[j] += s1_time - s_time\n",
    "\n",
    "for i in range(len(client_time)):\n",
    "    client_time[i] /= i_stop / 1000 # ms per frame\n",
    "\n",
    "# print the list without [ and ]\n",
    "out_string = str(client_time).replace('[','').replace(']','')\n",
    "print(dataset, model)\n",
    "print(out_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a image and store it\n",
    "# image = cv2.imread('../data/imagenet-client/0.bmp', cv2.IMREAD_COLOR)\n",
    "# # encode\n",
    "# image = image.astype(np.uint8)\n",
    "# image = base64.b64encode(image)\n",
    "# # store\n",
    "# with open('../data/imagenet-client/0b', 'wb') as f:\n",
    "#     f.write(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "data_root = '../data/imagenet-client/'\n",
    "# get '.bmp'\n",
    "import os \n",
    "images_list = os.listdir(data_root)\n",
    "images_list = [x for x in images_list if x.endswith('.bmp')]\n",
    "\n",
    "data_out = '../data/imagenet-jpeg/'\n",
    "\n",
    "import cv2\n",
    "# compress the image jpeg\n",
    "for i_path in images_list:\n",
    "    image_path = data_root + i_path\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    for quality in range(5, 100, 5):\n",
    "        cv2.imwrite(data_out+i_path[:-4]+'_'+str(quality)+'.jpg', image, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "def transform(self):\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        transform = transforms.Compose(\n",
    "                    [\n",
    "                        transforms.Resize(256),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean, std),\n",
    "                    ]\n",
    "                )\n",
    "        return transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
