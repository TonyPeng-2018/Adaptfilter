{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar-10, model: mobilenetV2, gated: True, ranker: entropy, generator: False, gatename: GateMLP\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'Models.ranker' has no attribute 'Ranker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 222\u001b[0m\n\u001b[1;32m    220\u001b[0m args \u001b[38;5;241m=\u001b[39m custom_args()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m--> 222\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     49\u001b[0m rankerlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     rankerlist\u001b[38;5;241m.\u001b[39mappend(\u001b[43mranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRanker\u001b[49m(args\u001b[38;5;241m.\u001b[39mranker))\n\u001b[1;32m     55\u001b[0m gateds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'Models.ranker' has no attribute 'Ranker'"
     ]
    }
   ],
   "source": [
    "# we use the CNN layer ranker\n",
    "from Dataloaders.dataloader_cifar10 import Dataloader_cifar10\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from Models import mobilenetv2\n",
    "from Utils import utils\n",
    "from Models import gatedmodel\n",
    "from Models import generator\n",
    "from Models import ranker\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    # initial using mobilenetV2, and cifar10\n",
    "    # we need a if statement here to decide which model and dataset to use\n",
    "    # random_seed = 2024\n",
    "\n",
    "    # for training, it is for training the generator \n",
    "    # recall the graph, when we cut more features, the performance should be worse.\n",
    "    \n",
    "    # get the training loader, freeze the model. Where is the partitioning point? \n",
    "    \"\"\"\n",
    "\n",
    "    # 1. get the train, test and val datasets, and labels.\n",
    "    if args.dataset == 'cifar-10':\n",
    "        # return train, test, val, labels, these are all dataloaders\n",
    "        _, test, classes = Dataloader_cifar10(train_batch=128, test_batch=100, seed=2024)\n",
    "    elif args.dataset == 'cifar-100':\n",
    "        pass\n",
    "    \n",
    "    # 2. transfer the dataset to fit the model, for the training, client and server model are all on the server\n",
    "    if args.model == 'mobilenetV2':\n",
    "        client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/model/MobileNetV2.pth')\n",
    "    elif args.model == 'resnet':\n",
    "        pass\n",
    "\n",
    "    # 3. get the gating, gating here decides how many channels are transferred to the server\n",
    "    # simple version: a binary tree, complex version: model\n",
    "    # get a ranker to rank the channels, and get the top k channels\n",
    "\n",
    "    # 4. get the gated model, we have 3 models here\n",
    "    gate_name = args.gatename\n",
    "    gated_rates = [0.25, 0.5, 0.75]\n",
    "    channel2ind = {8:0, 16:1, 24:2, 32:3}\n",
    "    input_channel = 32\n",
    "    rankerlist = []\n",
    "\n",
    "    for i in range(3):\n",
    "        rankerlist.append(ranker.Ranker_CNN1(input_channel, input_channel*gated_rates[i]))\n",
    "\n",
    "    gateds = []\n",
    "    for i in range(3):\n",
    "        gateds.append(gatedmodel.model_list[gate_name](\n",
    "            input_size=int(input_channel*gated_rates[i]),\n",
    "            width=32,\n",
    "            height=32,\n",
    "            output_size=10)) #  input_size, weight, height, output_size=10\n",
    "        s_time = utils.get_latest_weights(args.dataset, gate_name, 'gate')\n",
    "        gateds[i].load_state_dict(torch.load('./Weights/'+args.dataset+'/gate/'+gate_name+'_'+str(i)+'_'+s_time))\n",
    "\n",
    "    # 5. get the generator\n",
    "    generators = []\n",
    "    for i in range(3):\n",
    "        generators.append(generator.Generator(\n",
    "            inputsize=int(input_channel*gated_rates[i]), \n",
    "            hiddensize=32, \n",
    "            outputsize=32)) # inputsize, hiddensize, outputsize\n",
    "    \"\"\"\n",
    "    # 6. get the server \n",
    "    # server_model = some_model_function()\n",
    "    # server model is got above\n",
    "\n",
    "    # pipline data -> dataloader -> client_model -> gating -> reducer -> generator -> server_model\n",
    "    \n",
    "    # cuda may not have enough space for putting all the models\n",
    "    # client_model = client_model.cuda()\n",
    "    # server_model = server_model.cuda()\n",
    "    # for i in range(3):\n",
    "    #     gateds[i] = gateds[i].cuda()\n",
    "    #     generators[i] = generators[i].cuda() \"\"\"  \n",
    "\n",
    "    # set them to eval\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    for i in range(3):\n",
    "        gateds[i].eval()\n",
    "        generators[i].eval()\n",
    "\n",
    "    globbal_threshold = 0.8 # set a small value first\n",
    "\n",
    "    # load the test data set the test\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        t_conf = 0 # total confidence\n",
    "        t_zero = 0 # total number of 0\n",
    "        client_model = client_model.cuda()\n",
    "        server_model = server_model.cuda()\n",
    "        for data in test:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            # first, run the client model on the iot\n",
    "            out = client_model(images)\n",
    "            # calcualte the total number of 0\n",
    "            t_zero += torch.sum(out == 0).item()/32/32/32\n",
    "            # second, run the ranker\n",
    "            exit_flag = args.gated\n",
    "            counter = 0\n",
    "            # print('The number of 0 in the tensor is: ', torch.sum(out == 0).item())\n",
    "            # if the exit_flag is a True\n",
    "            if type(exit_flag) == bool:\n",
    "                while exit_flag:\n",
    "                    # get the emb and ind from the ranker\n",
    "                    out = out.cpu()\n",
    "                    s_emb, s_ind = ranker(out, gated_rates[counter])\n",
    "                    s_emb = s_emb.cuda()\n",
    "                    # give it to the gated model\n",
    "                    cur_gated = gateds[counter].cuda()\n",
    "                    g_emb = cur_gated(s_emb) # b, n\n",
    "                    # get the argmax\n",
    "                    g_conf = torch.max(g_emb, dim=1).values # b\n",
    "                    # use the sigmoid to get the confidence\n",
    "                    g_conf = torch.nn.functional.sigmoid(g_conf) # b\n",
    "                    # if in a batch, get the average\n",
    "                    g_conf = torch.mean(g_conf)\n",
    "                    print('The confidence is: ', g_conf.item())\n",
    "                    if g_conf > globbal_threshold:\n",
    "                        exit_flag = not exit_flag\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                # check the exit flag and send\n",
    "                if exit_flag != args.gated:\n",
    "                    print('The choosen gate is: ', counter)\n",
    "                    out = s_emb\n",
    "\n",
    "            # if the exit_flag is a number\n",
    "            elif type(exit_flag) == int:\n",
    "                out = out.cpu()\n",
    "                s_emb, s_ind = ranker(out, gated_rates[exit_flag])\n",
    "                s_emb = s_emb.cuda()\n",
    "                cur_gated = gateds[exit_flag].cuda()\n",
    "                g_emb = cur_gated(s_emb)\n",
    "                g_conf = torch.max(g_emb, dim=1).values\n",
    "                g_conf = torch.nn.functional.sigmoid(g_conf)\n",
    "                g_conf = torch.mean(g_conf)\n",
    "                # print('The confidence is: ', g_conf.item())\n",
    "                t_conf += g_conf.item()\n",
    "                out = s_emb # b, c, h, w\n",
    "                # print the number of 0 in the tensor)\n",
    "                # print('The number of 0 in the tensor is: ', torch.sum(out == 0).item())\n",
    "                # return 0\n",
    "\n",
    "            # a sender here, but on server, we don't have it.\n",
    "            \n",
    "            # a receiver here, but on server, we don't have it.\n",
    "            # get the generator\n",
    "            rec_size = out.size(1)\n",
    "            # print('The size of the tensor is: ', rec_size)\n",
    "            if exit_flag != args.gated or type(args.gated) == int: # int !=\n",
    "                rec_ind = s_ind\n",
    "            rec_size2ind = channel2ind[rec_size]\n",
    "            # if we don't get all features, we need to use the generator\n",
    "            if args.generator:\n",
    "                if rec_size2ind != 3:\n",
    "                    cur_gen = generators[rec_size2ind]\n",
    "                    # load weights\n",
    "                    cur_gen.load_state_dict(torch.load('./Weights/cifar-10/generator/generator_'+str(rec_size2ind)+'.pth'))\n",
    "                    cur_gen = cur_gen.cuda()\n",
    "                    out = cur_gen(out)\n",
    "\n",
    "            # skip the generator, create a tensor with all zeros\n",
    "            else:\n",
    "                if exit_flag != args.gated or type(args.gated) == int:\n",
    "                    n_out = torch.zeros(out.size(0), 32, 32, 32).cuda()\n",
    "                    n_out[:, rec_ind, :, :] = out\n",
    "                    out = n_out\n",
    "            # run the server model\n",
    "            out = server_model(out)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "        print('The average confidence is: ', 100 * t_conf/total)\n",
    "        print('The average accuracy is: ', 100 * correct / total)\n",
    "        print('The average number of 0 is: ', t_zero/total)   \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('enter')\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # we need the name of model, the name of dataset\n",
    "#     parser.add_argument('--dataset', type=str, default='cifar10', help='name of dataset')\n",
    "#     # parser.add_argument('--iot_model', type=str, default='mobilenetV2', help='name of the model on the iot')\n",
    "#     parser.add_argument('--reducer', type=str, default='entrophy', help='name of the reducer')\n",
    "#     parser.add_argument('--client', type=str, default='LTE', help='name of the network condition on the client side')\n",
    "#     parser.add_argument('--server', type=str, default='LTE', help='name of the network condition on the server side')\n",
    "#     parser.add_argument('--generator', type=str, default='None', help='name of the generator')\n",
    "#     # parser.add_argument('--server_model', type=str, default='mobilenetV2', help='name of the model on the server, should be the same as it on the iot')\n",
    "#     parser.add_argument('--device', type=str, default='home', help='run on which device, home, tintin, rpi, pico, jetson?')\n",
    "#     parser.add_argument('--model', type=str, default='mobilenetV2', help='name of the model')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n",
    "\n",
    "class custom_args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'cifar-10'\n",
    "        self.model = 'mobilenetV2'\n",
    "        self.gated = True\n",
    "        self.gatename = 'GateMLP'\n",
    "        self.ranker = 'entropy'\n",
    "        self.generator = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'dataset: '+self.dataset+', model: '+self.model+', gated: '+str(self.gated)+', ranker: '+self.ranker+', generator: '+str(self.generator) + ', gatename: '+self.gatename\n",
    "\n",
    "args = custom_args()\n",
    "print(args) \n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0261)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(1,10)\n",
    "b = torch.nn.functional.sigmoid(a)\n",
    "torch.sum(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
