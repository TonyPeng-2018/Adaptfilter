{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "from Dataloaders import dataloader_cifar10, dataloader_image_20\n",
    "from Models import gatedmodel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "models_name = {'mobile':mobilenetv2, 'resnet':resnet}\n",
    "datasets_name = {'cifar':dataloader_cifar10, 'imagenet':dataloader_image_20}\n",
    "middle_size_mobile = {x:2**x for x in range(5)}\n",
    "middle_size_resnet = {x:2**x for x in range(6)}\n",
    "middles_size = {'mobile':middle_size_mobile, 'resnet':middle_size_resnet}\n",
    "\n",
    "quantization = False\n",
    "int_trans = False\n",
    "device = 'cuda:0'\n",
    "\n",
    "def float_to_uint(x):\n",
    "    return torch.round(x * 255).int()\n",
    "\n",
    "def uint_to_float(x):\n",
    "    return x.float() / 255\n",
    "\n",
    "def MSE(x, y):\n",
    "    return torch.mean((x - y)**2)\n",
    "\n",
    "def KL_div(x, y):\n",
    "    return torch.mean(x * torch.log(x/y))\n",
    "\n",
    "def re_transform_cifar(x):\n",
    "    x = x * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def re_transform_imagenet(x):\n",
    "    x = x * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def transform_cifar(x):\n",
    "    x = (x - torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)) / torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def transform_imagenet(x):\n",
    "    x = (x - torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [00:02<03:25, 48.22it/s]\n",
      "  1%|          | 100/10000 [00:03<05:43, 28.81it/s]\n"
     ]
    }
   ],
   "source": [
    "width = 16\n",
    "height = 16\n",
    "client, server = mobilenetv2.mobilenetv2_splitter(num_classes=10, weight_root='./Weights/cifar-10')\n",
    "\n",
    "_, test, _ = dataloader_cifar10.Dataloader_cifar10_val(datasetpath='../data/', test_batch=1)\n",
    "_, test2, _ = dataloader_cifar10.Dataloader_cifar10_val(datasetpath='../data/', test_batch=1, normalize=False)\n",
    "\n",
    "middle_models = []\n",
    "for i in range (len(middle_size_mobile)):\n",
    "    middle = mobilenetv2.MobileNetV2_middle(middle=middle_size_mobile[i])\n",
    "    middle.load_state_dict(torch.load('./Weights/cifar-10/middle/mobile_cifar-10_middle_%s.pth'%str(middle_size_mobile[i])))\n",
    "    middle_models.append(middle)\n",
    "\n",
    "# eval and cuda\n",
    "client = client.eval().to(device)\n",
    "server = server.eval().to(device)\n",
    "for middle in middle_models:\n",
    "    middle = middle.eval().to(device)\n",
    "\n",
    "# use the quantization\n",
    "client = torch.ao.quantization.quantize_dynamic(client, dtype=torch.qint8)\n",
    "for middle in middle_models:\n",
    "    middle.in_layer = torch.ao.quantization.quantize_dynamic(middle.in_layer, dtype=torch.qint8)\n",
    "\n",
    "jpeg_list = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "middle_results = np.zeros((len(middle_models), len(test.dataset)))\n",
    "jpeg_results = np.zeros((len(jpeg_list), len(test.dataset)))\n",
    "\n",
    "stop_ind = 100\n",
    "with torch.no_grad():\n",
    "    # get the conf for each middles\n",
    "    correct_middle = np.zeros(len(middle_models))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "\n",
    "        original_div = None\n",
    "        data, target = data.to(device).detach(), target.to(device).detach()\n",
    "        out = client(data).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "\n",
    "        for j in range(len(middle_models)):\n",
    "            middle = middle_models[j]\n",
    "            out = client(data).detach()\n",
    "            out = middle.in_layer(out)\n",
    "            out = float_to_uint(out)\n",
    "            out = uint_to_float(out)\n",
    "            out = middle.out_layer(out)\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            middle_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_middle[j] += 1\n",
    "\n",
    "    correct_jpeg = np.zeros(len(jpeg_list))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test2)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "        data, target = data, target.to(device).detach()\n",
    "        out = transform_cifar(data)\n",
    "        data = data.to(device).detach()\n",
    "        out = out.to(device)\n",
    "        out = client(out).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "        for j in range(len(jpeg_list)):\n",
    "            jpeg = jpeg_list[j]\n",
    "            out = data.squeeze(0).cpu()\n",
    "            out = out.permute(1, 2, 0).numpy()\n",
    "            out = out * 255\n",
    "            out = out.astype(np.uint8)\n",
    "            # transform gbr to rgb\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "            # store out\n",
    "            # cv2.imwrite('temp.jpg', out)\n",
    "            out = cv2.imencode('.jpg', out, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])[1]\n",
    "            out = cv2.imdecode(out, cv2.IMREAD_COLOR)\n",
    "            # transfor rgb to gbr\n",
    "            # cv2.imwrite('temp%d.jpg'%j, out)\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n",
    "            out = out.astype(np.float16) / 255\n",
    "            out = torch.tensor(out).permute(2, 0, 1).unsqueeze(0)\n",
    "            out = transform_cifar(out).to(device)\n",
    "            out = client(out).detach()\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            # print if the original_div == out max\n",
    "            # print(torch.argmax(original_div) == torch.argmax(out))\n",
    "            jpeg_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_jpeg[j] += 1\n",
    "\n",
    "middle_pd = pd.DataFrame(middle_results.T, columns=['%d%%'%(middle_size_mobile[x]*100//32) for x in range(len(middle_size_mobile))])\n",
    "jpeg_pd = pd.DataFrame(jpeg_results.T, columns=[jpeg_list[x] for x in range(len(jpeg_list))])\n",
    "correct_jpeg = 1 - correct_jpeg / stop_ind\n",
    "correct_middle = 1 - correct_middle / stop_ind \n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=middle_pd, palette='Set3')\n",
    "plt.xlabel('Representation Percentage', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['3%', '6%', '12%', '25%', '50%']\n",
    "ax2.plot(temp_x, correct_middle, color='r', label='AVG MR', linewidth=2)\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.legend()\n",
    "plt.savefig('./Plots/middle_boxplot_cifar.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=jpeg_pd, palette='Set3')\n",
    "plt.xlabel('JPEG Quality', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['10', '20', '30', '40', '50', '60', '70', '80', '90']\n",
    "ax2.plot(temp_x, correct_jpeg, label='JPEG')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.savefig('./Plots/jpeg_boxplot_cifar.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5087 [00:02<02:24, 34.41it/s]\n",
      "  0%|          | 3/5087 [00:00<16:48,  5.04it/s]/home/tonypeng/anaconda3/envs/iot/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "  2%|▏         | 100/5087 [00:14<11:48,  7.04it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`legend` must be 'auto', 'brief', 'full', or a boolean.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m correct_middle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m correct_middle \u001b[38;5;241m/\u001b[39m stop_ind \n\u001b[1;32m    103\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m--> 104\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmiddle_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSet3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRepresentation Percentage\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    106\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKL Divergence\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/seaborn/categorical.py:1634\u001b[0m, in \u001b[0;36mboxplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m color \u001b[38;5;241m=\u001b[39m _default_color(\n\u001b[1;32m   1628\u001b[0m     ax\u001b[38;5;241m.\u001b[39mfill_between, hue, color,\n\u001b[1;32m   1629\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[1;32m   1630\u001b[0m     saturation\u001b[38;5;241m=\u001b[39msaturation,\n\u001b[1;32m   1631\u001b[0m )\n\u001b[1;32m   1632\u001b[0m linecolor \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39m_complement_color(linecolor, color, p\u001b[38;5;241m.\u001b[39m_hue_map)\n\u001b[0;32m-> 1634\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_boxes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdodge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdodge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfliersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfliersize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1647\u001b[0m p\u001b[38;5;241m.\u001b[39m_add_axis_labels(ax)\n\u001b[1;32m   1648\u001b[0m p\u001b[38;5;241m.\u001b[39m_adjust_cat_axis(ax, axis\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39morient)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/seaborn/categorical.py:745\u001b[0m, in \u001b[0;36m_CategoricalPlotter.plot_boxes\u001b[0;34m(self, width, dodge, gap, fill, whis, color, linecolor, linewidth, fliersize, plot_kws)\u001b[0m\n\u001b[1;32m    742\u001b[0m     ax\u001b[38;5;241m.\u001b[39madd_container(BoxPlotContainer(artists))\n\u001b[1;32m    744\u001b[0m legend_artist \u001b[38;5;241m=\u001b[39m _get_patch_legend_artist(fill)\n\u001b[0;32m--> 745\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_configure_legend\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend_artist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxprops\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/seaborn/categorical.py:427\u001b[0m, in \u001b[0;36m_CategoricalPlotter._configure_legend\u001b[0;34m(self, ax, func, common_kws, semantic_kws)\u001b[0m\n\u001b[1;32m    425\u001b[0m     show_legend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_legend:\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_legend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemantic_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_kws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     handles, _ \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_legend_handles_labels()\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles:\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/seaborn/_base.py:1209\u001b[0m, in \u001b[0;36mVectorPlotter.add_legend_data\u001b[0;34m(self, ax, func, common_kws, attrs, semantic_kws)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(verbosity, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m verbosity \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrief\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1208\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`legend` must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrief\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or a boolean.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m verbosity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: `legend` must be 'auto', 'brief', 'full', or a boolean."
     ]
    }
   ],
   "source": [
    "width = 16\n",
    "height = 16\n",
    "client, server = resnet.resnet_splitter(num_classes=1000, weight_root='./Weights/imagenet', layers=50)\n",
    "\n",
    "_, test, _ = dataloader_image_20.Dataloader_imagenet_20_integrated(test_batch=1)\n",
    "_, test2, _ = dataloader_image_20.Dataloader_imagenet_20_integrated(test_batch=1, transform=False)\n",
    "\n",
    "middle_models = []\n",
    "for i in range (len(middle_size_resnet)):\n",
    "    middle = resnet.resnet_middle(middle=middle_size_resnet[i])\n",
    "    middle.load_state_dict(torch.load('./Weights/imagenet/middle/resnet_imagenet_middle_%s.pth'%str(middle_size_resnet[i])))\n",
    "    middle_models.append(middle)\n",
    "\n",
    "# eval and cuda\n",
    "client = client.eval().to(device)\n",
    "server = server.eval().to(device)\n",
    "for middle in middle_models:\n",
    "    middle = middle.eval().to(device)\n",
    "\n",
    "# use the quantization\n",
    "client = torch.ao.quantization.quantize_dynamic(client, dtype=torch.qint8)\n",
    "for middle in middle_models:\n",
    "    middle.in_layer = torch.ao.quantization.quantize_dynamic(middle.in_layer, dtype=torch.qint8)\n",
    "\n",
    "jpeg_list = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "middle_results = np.zeros((len(middle_models), len(test.dataset)))\n",
    "jpeg_results = np.zeros((len(jpeg_list), len(test.dataset)))\n",
    "\n",
    "stop_ind = 100\n",
    "with torch.no_grad():\n",
    "    # get the conf for each middles\n",
    "    correct_middle = np.zeros(len(middle_models))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "\n",
    "        original_div = None\n",
    "        data, target = data.to(device).detach(), target.to(device).detach()\n",
    "        out = client(data).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "\n",
    "        for j in range(len(middle_models)):\n",
    "            middle = middle_models[j]\n",
    "            out = client(data).detach()\n",
    "            out = middle.in_layer(out)\n",
    "            out = float_to_uint(out)\n",
    "            out = uint_to_float(out)\n",
    "            out = middle.out_layer(out)\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            middle_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_middle[j] += 1\n",
    "\n",
    "    correct_jpeg = np.zeros(len(jpeg_list))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test2)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "        data, target = data, target.to(device).detach()\n",
    "        out = transform_cifar(data)\n",
    "        data = data.to(device).detach()\n",
    "        out = out.to(device)\n",
    "        out = client(out).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "        for j in range(len(jpeg_list)):\n",
    "            jpeg = jpeg_list[j]\n",
    "            out = data.squeeze(0).cpu()\n",
    "            out = out.permute(1, 2, 0).numpy()\n",
    "            out = out * 255\n",
    "            out = out.astype(np.uint8)\n",
    "            # transform gbr to rgb\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "            # store out\n",
    "            # cv2.imwrite('temp.jpg', out)\n",
    "            out = cv2.imencode('.jpg', out, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])[1]\n",
    "            out = cv2.imdecode(out, cv2.IMREAD_COLOR)\n",
    "            # transfor rgb to gbr\n",
    "            # cv2.imwrite('temp%d.jpg'%j, out)\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n",
    "            out = out.astype(np.float16) / 255\n",
    "            out = torch.tensor(out).permute(2, 0, 1).unsqueeze(0)\n",
    "            out = transform_cifar(out).to(device)\n",
    "            out = client(out).detach()\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            # print if the original_div == out max\n",
    "            # print(torch.argmax(original_div) == torch.argmax(out))\n",
    "            jpeg_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_jpeg[j] += 1\n",
    "\n",
    "middle_pd = pd.DataFrame(middle_results.T, columns=['%d%%'%(middle_size_resnet[x]*100//64) for x in range(len(middle_size_resnet))])\n",
    "jpeg_pd = pd.DataFrame(jpeg_results.T, columns=[jpeg_list[x] for x in range(len(jpeg_list))])\n",
    "correct_jpeg = 1 - correct_jpeg / stop_ind\n",
    "correct_middle = 1 - correct_middle / stop_ind \n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=middle_pd, palette='Set3', legend='samples')\n",
    "plt.xlabel('Representation Percentage', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "# sns.set_theme(style=\"white\", palette=None )\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['1%', '3%', '6%', '12%', '25%', '50%']\n",
    "ax2.plot(temp_x, correct_middle, label='Middle', width=0.5, legend='AVG MR')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.legend()\n",
    "plt.savefig('./Plots/middle_boxplot_imagenet.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=jpeg_pd, palette='Set3')\n",
    "plt.xlabel('JPEG Quality', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['10', '20', '30', '40', '50', '60', '70', '80', '90']\n",
    "ax2.plot(temp_x, correct_jpeg, label='JPEG')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.savefig('./Plots/jpeg_boxplot_imagenet.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
