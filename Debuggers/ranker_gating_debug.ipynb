{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# this is the first version ranker\\n# first, load the embeddings\\n# stored in torch tensors\\n\\nimport torch\\nimport numpy as np\\nimport os\\nimport sys\\nimport json\\nimport argparse\\nimport time\\nimport random\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.autograd import Variable\\nimport scipy.stats \\n\\nembeddings_folder = '../data/cifar-10-embedding-3/embeddings/' \\nembeddings_files = sorted(os.listdir(embeddings_folder))\\ntest_embeddings = torch.load(embeddings_folder + embeddings_files[0]) # load the first file\\nprint(test_embeddings.size()) # 128, 32, 32, 32\\n\\n# calculate the entropy of the embeddings, select 1%, 5%, 10%, 20%, 50% of the embeddings\\ndef calculate_entropy(embs, percentage):\\n    # embs are torch tensors\\n    # percentage shows the number of embeddings to select\\n    embs_entropy = []\\n    for i in range(embs.size(len(embs.size())-3)):\\n        embs_entropy.append(scipy.stats.entropy(embs[:, i].reshape(-1))) # b, c\\n    # get the top percentage of the channels\\n    embs_entropy = np.array(embs_entropy)\\n    indices = np.argsort(embs_entropy)\\n    num_selected = int(embs.size(1) * percentage)\\n    selected_indices = indices[:num_selected]\\n    return selected_indices\\n    # out b, c*p, h, w or c*p, h, w\\n\\ndef ranker_entropy(embs, percentage):\\n    # calculate the entropy of the embeddings\\n    selected_indices = calculate_entropy(embs, percentage)\\n    # select indice from test embeddings\\n    selected_embeddings = embs[:, selected_indices] # b, c*p, h, w\\n    return selected_embeddings\\n\\n# get the selected embeddings\\nselected_embeddings = ranker_entropy(test_embeddings, 0.1)\\nprint(selected_embeddings.size())\\n\\n# design a simple gating mechanism for regression\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# this is the first version ranker\n",
    "# first, load the embeddings\n",
    "# stored in torch tensors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats \n",
    "\n",
    "embeddings_folder = '../data/cifar-10-embedding-3/embeddings/' \n",
    "embeddings_files = sorted(os.listdir(embeddings_folder))\n",
    "test_embeddings = torch.load(embeddings_folder + embeddings_files[0]) # load the first file\n",
    "print(test_embeddings.size()) # 128, 32, 32, 32\n",
    "\n",
    "# calculate the entropy of the embeddings, select 1%, 5%, 10%, 20%, 50% of the embeddings\n",
    "def calculate_entropy(embs, percentage):\n",
    "    # embs are torch tensors\n",
    "    # percentage shows the number of embeddings to select\n",
    "    embs_entropy = []\n",
    "    for i in range(embs.size(len(embs.size())-3)):\n",
    "        embs_entropy.append(scipy.stats.entropy(embs[:, i].reshape(-1))) # b, c\n",
    "    # get the top percentage of the channels\n",
    "    embs_entropy = np.array(embs_entropy)\n",
    "    indices = np.argsort(embs_entropy)\n",
    "    num_selected = int(embs.size(1) * percentage)\n",
    "    selected_indices = indices[:num_selected]\n",
    "    return selected_indices\n",
    "    # out b, c*p, h, w or c*p, h, w\n",
    "\n",
    "def ranker_entropy(embs, percentage):\n",
    "    # calculate the entropy of the embeddings\n",
    "    selected_indices = calculate_entropy(embs, percentage)\n",
    "    # select indice from test embeddings\n",
    "    selected_embeddings = embs[:, selected_indices] # b, c*p, h, w\n",
    "    return selected_embeddings\n",
    "\n",
    "# get the selected embeddings\n",
    "selected_embeddings = ranker_entropy(test_embeddings, 0.1)\n",
    "print(selected_embeddings.size())\n",
    "\n",
    "# design a simple gating mechanism for regression\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats \n",
    "\n",
    "def calculate_entropy(embs, percentage):\n",
    "    # embs are torch tensors\n",
    "    # percentage shows the number of embeddings to select\n",
    "    embs_entropy = []\n",
    "    for i in range(embs.size(len(embs.size())-3)):\n",
    "        embs_entropy.append(scipy.stats.entropy(embs[:, i].reshape(-1))) # b, c\n",
    "    # get the top percentage of the channels\n",
    "    embs_entropy = np.array(embs_entropy)\n",
    "    indices = np.argsort(embs_entropy)\n",
    "    num_selected = int(embs.size(1) * percentage)\n",
    "    selected_indices = indices[:num_selected]\n",
    "    return selected_indices\n",
    "    # out b, c*p, h, w or c*p, h, w\n",
    "\n",
    "def ranker_entropy(embs, percentage):\n",
    "    # calculate the entropy of the embeddings\n",
    "    selected_indices = calculate_entropy(embs, percentage)\n",
    "    # select indice from test embeddings\n",
    "    selected_embeddings = embs[:, selected_indices] # b, c*p, h, w\n",
    "    return selected_embeddings, selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats\n",
    "import torch.utils \n",
    "import os\n",
    "from Models import mobilenetv2\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:46<37:40, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 12.1612, Gated: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:56<45:49, 56.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# get the selected embeddings\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, rate \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f_rate):\n\u001b[0;32m--> 138\u001b[0m     s_emb, s_ind \u001b[38;5;241m=\u001b[39m \u001b[43mranker_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# make the embeddings to fit the server model\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# n_emb = torch.zeros(s_emb.size(0), 32, 32, 32)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# n_emb[:, s_ind] = s_emb\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# n_emb = n_emb.cuda()\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# multiple gated regression models, we use the selected embeddings\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     s_emb \u001b[38;5;241m=\u001b[39m s_emb\u001b[38;5;241m.\u001b[39mcuda()\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mranker_entropy\u001b[0;34m(embs, percentage)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mranker_entropy\u001b[39m(embs, percentage):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# calculate the entropy of the embeddings\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     selected_indices \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# select indice from test embeddings\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     selected_embeddings \u001b[38;5;241m=\u001b[39m embs[:, selected_indices] \u001b[38;5;66;03m# b, c*p, h, w\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mcalculate_entropy\u001b[0;34m(embs, percentage)\u001b[0m\n\u001b[1;32m     17\u001b[0m embs_entropy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(embs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mlen\u001b[39m(embs\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)):\n\u001b[0;32m---> 19\u001b[0m     embs_entropy\u001b[38;5;241m.\u001b[39mappend(\u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# b, c\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# get the top percentage of the channels\u001b[39;00m\n\u001b[1;32m     21\u001b[0m embs_entropy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embs_entropy)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentinel:\n\u001b[1;32m    530\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _remove_sentinel(samples, paired, sentinel)\n\u001b[0;32m--> 531\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    533\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/scipy/stats/_entropy.py:146\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(pk, qk, base, axis)\u001b[0m\n\u001b[1;32m    144\u001b[0m     pk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39mpk \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(pk, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mspecial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     qk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(qk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import the server model\n",
    "run_device = 'home'\n",
    "# This is the version 1 gated regression model, we need a simpler version\n",
    "# class GatedRegression(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(GatedRegression, self).__init__()\n",
    "#         # think about it 3*32*32 -> 1\n",
    "#         # think about the classification of the mobile net\n",
    "#         self.input_size = input_size\n",
    "#         self.num_classes = num_classes\n",
    "#         self.hidden_size = input_size\n",
    "#         self.conv1 = nn.Conv2d(input_size, 2*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(2*self.hidden_size)\n",
    "#         self.conv2 = nn.Conv2d(2*self.hidden_size, 4*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(4*self.hidden_size)\n",
    "#         self.conv3 = nn.Conv2d(4*self.hidden_size, 8*self.hidden_size, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(8*self.hidden_size)\n",
    "#         self.linear = nn.Linear(8*self.hidden_size, num_classes)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.avgpool = nn.AvgPool2d(4)\n",
    "#         self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # need to change 32 -> 4\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.avgpool(out)\n",
    "#         out = self.flatten(out)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "\n",
    "class GatedRegression(nn.Module):\n",
    "    def __init__(self, input_size, weight, height, output_size=10):\n",
    "        super(GatedRegression, self).__init__()\n",
    "        # think about it 3*32*32 -> 1\n",
    "        # think about the classification of the mobile net\n",
    "        # the input size is b, c*p, h, w, the output size is b \n",
    "        # how to make sure more features help the server model?\n",
    "\n",
    "        self.input_size = input_size * weight * height # 8, 32, 32 - 24, 32, 32\n",
    "        self.output_size = output_size\n",
    "        # 1280 = 5*16*16\n",
    "        self.structure = [self.input_size, self.input_size//16, self.output_size]\n",
    "        self.linear1 = nn.Linear(self.structure[0], self.structure[1])\n",
    "        self.linear2 = nn.Linear(self.structure[1], self.structure[2])\n",
    "        # self.linear3 = nn.Linear(self.structure[2], self.structure[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # need to change it to 0-1\n",
    "        # flatten the input first\n",
    "        out = self.flatten(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# load the dataset\n",
    "class gated_dataset(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, embeddings_folder, labels_folder):\n",
    "        self.embeddings_folder = embeddings_folder\n",
    "        self.embeddings_files = sorted(os.listdir(embeddings_folder+'embeddings/'))\n",
    "        self.labels_folder = labels_folder\n",
    "        self.labels_files = sorted(os.listdir(labels_folder+'embeddings/'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings_files.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.embeddings = torch.load(self.embeddings_folder+'embeddings/' + self.embeddings_files[idx])\n",
    "        self.labels = torch.load(self.labels_folder+'embeddings/' + self.labels_files[idx])\n",
    "        return self.embeddings, self.labels\n",
    "    \n",
    "# load the dataset\n",
    "if run_device == 'tintin':\n",
    "    embeddings_folder = '/data/anp407/cifar-10-embedding-3/'\n",
    "    labels_folder = '/data/anp407/cifar-10-embedding-out/'\n",
    "if run_device == 'home':\n",
    "    embeddings_folder = '../data/cifar-10-embedding-3/'\n",
    "    labels_folder = '../data/cifar-10-embedding-out/'\n",
    "dataset = gated_dataset(embeddings_folder, labels_folder)\n",
    "# print(gated_dataset.__len__(dataset)) # 391 batches                                             \n",
    "\n",
    "# load the data loader, train no test\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# load the model\n",
    "Gateds = []\n",
    "input_sizes = [8, 16, 24]\n",
    "# added Gated to Gateds\n",
    "for input_size in input_sizes:\n",
    "    Gated = GatedRegression(input_size=input_size, weight=32, height=32, output_size=10)\n",
    "    Gateds.append(Gated)\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "# model settings\n",
    "for Gated in Gateds:\n",
    "    Gated = Gated.cuda()\n",
    "    Gated.train()\n",
    "\n",
    "# get the server model\n",
    "client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "server_model = server_model.cuda()\n",
    "server_model.eval()\n",
    "# load the optimizer\n",
    "optimizer = optim.Adam(Gated.parameters(), lr=0.001)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "\n",
    "# train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        embs, labels = data\n",
    "        # for the training, embeddings doesn't need to be on the cuda\n",
    "        # embs, labels = embs.cuda(), labels.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # embeddings = Variable(embeddings) # what are they?\n",
    "        # labels = Variable(labels) # what are they?\n",
    "        embs = embs.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        # for the training, we make 4 gated regression models\n",
    "        f_rate = [0.25, 0.5, 0.75] # filter rates\n",
    "        # s_embs, s_inds = [], []\n",
    "        preds = []\n",
    "        # get the selected embeddings\n",
    "        for j, rate in enumerate(f_rate):\n",
    "            s_emb, s_ind = ranker_entropy(embs, rate)\n",
    "            # make the embeddings to fit the server model\n",
    "            # n_emb = torch.zeros(s_emb.size(0), 32, 32, 32)\n",
    "            # n_emb[:, s_ind] = s_emb\n",
    "            # n_emb = n_emb.cuda()\n",
    "            # multiple gated regression models, we use the selected embeddings\n",
    "            s_emb = s_emb.cuda()\n",
    "            preds.append(Gateds[j](s_emb))\n",
    "\n",
    "        # get the output from the server model\n",
    "        # with torch.no_grad():\n",
    "        #     embs = embs.cuda()\n",
    "        #     outputs = server_model(embs)\n",
    "        # get the save result \n",
    "\n",
    "\n",
    "        # loss backpropagation\n",
    "        losses = [0] * len(preds)\n",
    "        for j, pred in enumerate(preds):\n",
    "            loss = criterion(pred, labels)\n",
    "            losses[j] = loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    for j, loss in enumerate(losses):\n",
    "        print('Epoch [%d/%d], Loss: %.4f, Gated: %d' % (epoch+1, epochs, loss.item(), j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section is for training the gate\n",
    "# CNN and 2-CNN\n",
    "run_device = 'home'\n",
    "\n",
    "class GatedRegression(nn.Module):\n",
    "    def __init__(self, input_size, weight, height, output_size=10):\n",
    "        super(GatedRegression, self).__init__()\n",
    "        # think about it 3*32*32 -> 1\n",
    "        # think about the classification of the mobile net\n",
    "        # the input size is b, c*p, h, w, the output size is b \n",
    "        # how to make sure more features help the server model?\n",
    "\n",
    "        self.input_size = input_size * weight * height # 8, 32, 32 - 24, 32, 32\n",
    "        self.output_size = output_size\n",
    "        # 1280 = 5*16*16\n",
    "        self.structure = [self.input_size, self.input_size//16, self.output_size]\n",
    "        self.linear1 = nn.Linear(self.structure[0], self.structure[1])\n",
    "        self.linear2 = nn.Linear(self.structure[1], self.structure[2])\n",
    "        # self.linear3 = nn.Linear(self.structure[2], self.structure[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # need to change it to 0-1\n",
    "        # flatten the input first\n",
    "        out = self.flatten(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# load the dataset\n",
    "class gated_dataset(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, embeddings_folder, labels_folder):\n",
    "        self.embeddings_folder = embeddings_folder\n",
    "        self.embeddings_files = sorted(os.listdir(embeddings_folder+'embeddings/'))\n",
    "        self.labels_folder = labels_folder\n",
    "        self.labels_files = sorted(os.listdir(labels_folder+'embeddings/'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings_files.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.embeddings = torch.load(self.embeddings_folder+'embeddings/' + self.embeddings_files[idx])\n",
    "        self.labels = torch.load(self.labels_folder+'embeddings/' + self.labels_files[idx])\n",
    "        return self.embeddings, self.labels\n",
    "    \n",
    "# load the dataset\n",
    "if run_device == 'tintin':\n",
    "    embeddings_folder = '/data/anp407/cifar-10-embedding-3/'\n",
    "    labels_folder = '/data/anp407/cifar-10-embedding-out/'\n",
    "if run_device == 'home':\n",
    "    embeddings_folder = '../data/cifar-10-embedding-3/'\n",
    "    labels_folder = '../data/cifar-10-embedding-out/'\n",
    "dataset = gated_dataset(embeddings_folder, labels_folder)\n",
    "# print(gated_dataset.__len__(dataset)) # 391 batches                                             \n",
    "\n",
    "# load the data loader, train no test\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# load the model\n",
    "Gateds = []\n",
    "input_sizes = [8, 16, 24]\n",
    "# added Gated to Gateds\n",
    "for input_size in input_sizes:\n",
    "    Gated = GatedRegression(input_size=input_size, weight=32, height=32, output_size=10)\n",
    "    Gateds.append(Gated)\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "# model settings\n",
    "for Gated in Gateds:\n",
    "    Gated = Gated.cuda()\n",
    "    Gated.train()\n",
    "\n",
    "# get the server model\n",
    "client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "server_model = server_model.cuda()\n",
    "server_model.eval()\n",
    "# load the optimizer\n",
    "optimizer = optim.Adam(Gated.parameters(), lr=0.001)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "\n",
    "# train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        embs, labels = data\n",
    "        # for the training, embeddings doesn't need to be on the cuda\n",
    "        # embs, labels = embs.cuda(), labels.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # embeddings = Variable(embeddings) # what are they?\n",
    "        # labels = Variable(labels) # what are they?\n",
    "        embs = embs.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "        # for the training, we make 4 gated regression models\n",
    "        f_rate = [0.25, 0.5, 0.75] # filter rates\n",
    "        # s_embs, s_inds = [], []\n",
    "        preds = []\n",
    "        # get the selected embeddings\n",
    "        for j, rate in enumerate(f_rate):\n",
    "            s_emb, s_ind = ranker_entropy(embs, rate)\n",
    "            # make the embeddings to fit the server model\n",
    "            # n_emb = torch.zeros(s_emb.size(0), 32, 32, 32)\n",
    "            # n_emb[:, s_ind] = s_emb\n",
    "            # n_emb = n_emb.cuda()\n",
    "            # multiple gated regression models, we use the selected embeddings\n",
    "            s_emb = s_emb.cuda()\n",
    "            preds.append(Gateds[j](s_emb))\n",
    "\n",
    "        # get the output from the server model\n",
    "        # with torch.no_grad():\n",
    "        #     embs = embs.cuda()\n",
    "        #     outputs = server_model(embs)\n",
    "        # get the save result \n",
    "\n",
    "\n",
    "        # loss backpropagation\n",
    "        losses = [0] * len(preds)\n",
    "        for j, pred in enumerate(preds):\n",
    "            loss = criterion(pred, labels)\n",
    "            losses[j] = loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    for j, loss in enumerate(losses):\n",
    "        print('Epoch [%d/%d], Loss: %.4f, Gated: %d' % (epoch+1, epochs, loss.item(), j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
