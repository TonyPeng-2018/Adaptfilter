{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar10, model: mobilenetV2, gated: False, ranker: entropy, generator: False\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Accuracy of the network on the 10000 test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "# this is the total pipeline for the project\n",
    "# This file is for trainning\n",
    "# Run this on the server, or as we called offline. \n",
    "\n",
    "# with gated, no generator\n",
    "from Dataloaders.dataloader_cifar10 import Dataloader_cifar10\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from Models import mobilenetv2\n",
    "from Utils import utils\n",
    "from Models import gatedmodel\n",
    "from Models import generator\n",
    "\n",
    "def main(args):\n",
    "    # initial using mobilenetV2, and cifar10\n",
    "    # we need a if statement here to decide which model and dataset to use\n",
    "    # random_seed = 2024\n",
    "\n",
    "    # for training, it is for training the generator \n",
    "    # recall the graph, when we cut more features, the performance should be worse.\n",
    "    \n",
    "    # get the training loader, freeze the model. Where is the partitioning point? \n",
    "\n",
    "    # 1. get the train, test and val datasets, and labels.\n",
    "    if args.dataset == 'cifar10':\n",
    "        # return train, test, val, labels, these are all dataloaders\n",
    "        _, test, classes = Dataloader_cifar10(train_batch=128, test_batch=100, seed=2024)\n",
    "    elif args.dataset == 'cifar100':\n",
    "        pass\n",
    "    \n",
    "    # 2. transfer the dataset to fit the model, for the training, client and server model are all on the server\n",
    "    if args.model == 'mobilenetV2':\n",
    "        client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "    elif args.model == 'resnet':\n",
    "        pass\n",
    "\n",
    "    # 3. get the gating, gating here decides how many channels are transferred to the server\n",
    "    # simple version: a binary tree, complex version: model\n",
    "    # get a ranker to rank the channels, and get the top k channels\n",
    "    ranker = utils.ranker_entropy # input: embs, percentage, output: s_emb, s_ind\n",
    "\n",
    "    # 4. get the gated model, we have 3 models here\n",
    "    gated_rates = [0.25, 0.5, 0.75]\n",
    "    channel2ind = {8:0, 16:1, 24:2, 32:3}\n",
    "    input_channel = 32\n",
    "    gateds = []\n",
    "    for i in range(3):\n",
    "        gateds.append(gatedmodel.GatedRegression(\n",
    "            input_size=int(input_channel*gated_rates[i]),\n",
    "            weight=32,\n",
    "            height=32,\n",
    "            output_size=10)) #  input_size, weight, height, output_size=10\n",
    "\n",
    "    # 5. get the generator\n",
    "    generators = []\n",
    "    for i in range(3):\n",
    "        generators.append(generator.Generator(\n",
    "            inputsize=int(input_channel*gated_rates[i]), \n",
    "            hiddensize=32, \n",
    "            outputsize=32)) # inputsize, hiddensize, outputsize\n",
    "        \n",
    "    # 6. get the server \n",
    "    # server_model = some_model_function()\n",
    "    # server model is got above\n",
    "\n",
    "    # pipline data -> dataloader -> client_model -> gating -> reducer -> generator -> server_model\n",
    "    \n",
    "    # cuda may not have enough space for putting all the models\n",
    "    # client_model = client_model.cuda()\n",
    "    # server_model = server_model.cuda()\n",
    "    # for i in range(3):\n",
    "    #     gateds[i] = gateds[i].cuda()\n",
    "    #     generators[i] = generators[i].cuda()\n",
    "\n",
    "    # set them to eval\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    for i in range(3):\n",
    "        gateds[i].eval()\n",
    "        generators[i].eval()\n",
    "\n",
    "    globbal_threshold = 0.8 # set a small value first\n",
    "\n",
    "    # load the test data set the test\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        client_model = client_model.cuda()\n",
    "        server_model = server_model.cuda()\n",
    "        for data in test:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            # first, run the client model on the iot\n",
    "            out = client_model(images)\n",
    "            # second, run the ranker\n",
    "            exit_flag = args.gated\n",
    "            counter = 0\n",
    "            while exit_flag:\n",
    "                # get the emb and ind from the ranker\n",
    "                out = out.cpu()\n",
    "                s_emb, s_ind = ranker(out, gated_rates[counter])\n",
    "                s_emb = s_emb.cuda()\n",
    "                # give it to the gated model\n",
    "                cur_gated = gateds[counter]\n",
    "                # load the weights\n",
    "                cur_gated.load_state_dict(torch.load('./Weights/cifar-10/GatedRegression_'+str(counter)+'.pth'))\n",
    "                cur_gated = cur_gated.cuda()\n",
    "                g_emb = cur_gated(s_emb) # b, n\n",
    "                # get the argmax\n",
    "                g_conf = torch.max(g_emb, dim=1).values # b\n",
    "                # use the sigmoid to get the confidence\n",
    "                g_conf = torch.nn.functional.sigmoid(g_conf) # b\n",
    "                # if in a batch, get the average\n",
    "                g_conf = torch.mean(g_conf)\n",
    "                # print('The confidence is: ', g_conf.item())\n",
    "                if g_conf > globbal_threshold:\n",
    "                    exit_flag = not exit_flag\n",
    "                else:\n",
    "                    counter += 1\n",
    "\n",
    "            # check the exit flag and send\n",
    "            if exit_flag != args.gated:\n",
    "                out = s_emb\n",
    "\n",
    "            # a sender here, but on server, we don't have it.\n",
    "            \n",
    "            # a receiver here, but on server, we don't have it.\n",
    "            # get the generator\n",
    "            rec_size = out.size(1)\n",
    "            if exit_flag != args.gated:\n",
    "                rec_ind = s_ind\n",
    "            rec_size2ind = channel2ind[rec_size]\n",
    "            # if we don't get all features, we need to use the generator\n",
    "\n",
    "            if args.generator:\n",
    "                if rec_size2ind != 3:\n",
    "                    cur_gen = generators[rec_size2ind]\n",
    "                    # load weights\n",
    "                    cur_gen.load_state_dict(torch.load('./Weights/cifar-10/generator_'+str(rec_size2ind)+'.pth'))\n",
    "                    cur_gen = cur_gen.cuda()\n",
    "                    out = cur_gen(out)\n",
    "\n",
    "            # skip the generator, create a tensor with all zeros\n",
    "            else:\n",
    "                if exit_flag != args.gated:\n",
    "                    n_out = torch.zeros(out.size(0), 32, 32, 32).cuda()\n",
    "                    n_out[:, rec_ind, :, :] = out\n",
    "                    out = n_out\n",
    "\n",
    "            # run the server model\n",
    "            out = server_model(out)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('enter')\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # we need the name of model, the name of dataset\n",
    "#     parser.add_argument('--dataset', type=str, default='cifar10', help='name of dataset')\n",
    "#     # parser.add_argument('--iot_model', type=str, default='mobilenetV2', help='name of the model on the iot')\n",
    "#     parser.add_argument('--reducer', type=str, default='entrophy', help='name of the reducer')\n",
    "#     parser.add_argument('--client', type=str, default='LTE', help='name of the network condition on the client side')\n",
    "#     parser.add_argument('--server', type=str, default='LTE', help='name of the network condition on the server side')\n",
    "#     parser.add_argument('--generator', type=str, default='None', help='name of the generator')\n",
    "#     # parser.add_argument('--server_model', type=str, default='mobilenetV2', help='name of the model on the server, should be the same as it on the iot')\n",
    "#     parser.add_argument('--device', type=str, default='home', help='run on which device, home, tintin, rpi, pico, jetson?')\n",
    "#     parser.add_argument('--model', type=str, default='mobilenetV2', help='name of the model')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n",
    "\n",
    "class custom_args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'cifar10'\n",
    "        self.model = 'mobilenetV2'\n",
    "        self.gated = False\n",
    "        self.ranker = 'entropy'\n",
    "        self.generator = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'dataset: '+self.dataset+', model: '+self.model+', gated: '+str(self.gated)+', ranker: '+self.ranker+', generator: '+str(self.generator)\n",
    "\n",
    "args = custom_args()\n",
    "print(args)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar10, model: mobilenetV2, gated: True, ranker: entropy, generator: False\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "Accuracy of the network on the 10000 test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "# this is the total pipeline for the project\n",
    "# This file is for trainning\n",
    "# Run this on the server, or as we called offline. \n",
    "\n",
    "# with gated, no generator\n",
    "from Dataloaders.dataloader_cifar10 import Dataloader_cifar10\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from Models import mobilenetv2\n",
    "from Utils import utils\n",
    "from Models import gatedmodel\n",
    "from Models import generator\n",
    "\n",
    "def main(args):\n",
    "    # initial using mobilenetV2, and cifar10\n",
    "    # we need a if statement here to decide which model and dataset to use\n",
    "    # random_seed = 2024\n",
    "\n",
    "    # for training, it is for training the generator \n",
    "    # recall the graph, when we cut more features, the performance should be worse.\n",
    "    \n",
    "    # get the training loader, freeze the model. Where is the partitioning point? \n",
    "\n",
    "    # 1. get the train, test and val datasets, and labels.\n",
    "    if args.dataset == 'cifar10':\n",
    "        # return train, test, val, labels, these are all dataloaders\n",
    "        _, test, classes = Dataloader_cifar10(train_batch=128, test_batch=100, seed=2024)\n",
    "    elif args.dataset == 'cifar100':\n",
    "        pass\n",
    "    \n",
    "    # 2. transfer the dataset to fit the model, for the training, client and server model are all on the server\n",
    "    if args.model == 'mobilenetV2':\n",
    "        client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "    elif args.model == 'resnet':\n",
    "        pass\n",
    "\n",
    "    # 3. get the gating, gating here decides how many channels are transferred to the server\n",
    "    # simple version: a binary tree, complex version: model\n",
    "    # get a ranker to rank the channels, and get the top k channels\n",
    "    ranker = utils.ranker_entropy # input: embs, percentage, output: s_emb, s_ind\n",
    "\n",
    "    # 4. get the gated model, we have 3 models here\n",
    "    gated_rates = [0.25, 0.5, 0.75]\n",
    "    channel2ind = {8:0, 16:1, 24:2, 32:3}\n",
    "    input_channel = 32\n",
    "    gateds = []\n",
    "    for i in range(3):\n",
    "        gateds.append(gatedmodel.GatedRegression(\n",
    "            input_size=int(input_channel*gated_rates[i]),\n",
    "            weight=32,\n",
    "            height=32,\n",
    "            output_size=10)) #  input_size, weight, height, output_size=10\n",
    "\n",
    "    # 5. get the generator\n",
    "    generators = []\n",
    "    for i in range(3):\n",
    "        generators.append(generator.Generator(\n",
    "            inputsize=int(input_channel*gated_rates[i]), \n",
    "            hiddensize=32, \n",
    "            outputsize=32)) # inputsize, hiddensize, outputsize\n",
    "        \n",
    "    # 6. get the server \n",
    "    # server_model = some_model_function()\n",
    "    # server model is got above\n",
    "\n",
    "    # pipline data -> dataloader -> client_model -> gating -> reducer -> generator -> server_model\n",
    "    \n",
    "    # cuda may not have enough space for putting all the models\n",
    "    # client_model = client_model.cuda()\n",
    "    # server_model = server_model.cuda()\n",
    "    # for i in range(3):\n",
    "    #     gateds[i] = gateds[i].cuda()\n",
    "    #     generators[i] = generators[i].cuda()\n",
    "\n",
    "    # set them to eval\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    for i in range(3):\n",
    "        gateds[i].eval()\n",
    "        generators[i].eval()\n",
    "\n",
    "    globbal_threshold = 0.8 # set a small value first\n",
    "\n",
    "    # load the test data set the test\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        client_model = client_model.cuda()\n",
    "        server_model = server_model.cuda()\n",
    "        for data in test:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            # first, run the client model on the iot\n",
    "            out = client_model(images)\n",
    "            # second, run the ranker\n",
    "            exit_flag = args.gated\n",
    "            counter = 0\n",
    "            while exit_flag:\n",
    "                # get the emb and ind from the ranker\n",
    "                out = out.cpu()\n",
    "                s_emb, s_ind = ranker(out, gated_rates[counter])\n",
    "                s_emb = s_emb.cuda()\n",
    "                # give it to the gated model\n",
    "                cur_gated = gateds[counter]\n",
    "                # load the weights\n",
    "                cur_gated.load_state_dict(torch.load('./Weights/cifar-10/GatedRegression_'+str(counter)+'.pth'))\n",
    "                cur_gated = cur_gated.cuda()\n",
    "                g_emb = cur_gated(s_emb) # b, n\n",
    "                # get the argmax\n",
    "                g_conf = torch.max(g_emb, dim=1).values # b\n",
    "                # use the sigmoid to get the confidence\n",
    "                g_conf = torch.nn.functional.sigmoid(g_conf) # b\n",
    "                # if in a batch, get the average\n",
    "                g_conf = torch.mean(g_conf)\n",
    "                # print('The confidence is: ', g_conf.item())\n",
    "                if g_conf > globbal_threshold:\n",
    "                    exit_flag = not exit_flag\n",
    "                else:\n",
    "                    counter += 1\n",
    "\n",
    "            # check the exit flag and send\n",
    "            if exit_flag != args.gated:\n",
    "                print('The choosen gate is: ', counter, 'The confidence is: ', g_conf.item(), 'The size of channel is: ', s_emb.size(1))\n",
    "                out = s_emb\n",
    "\n",
    "            # a sender here, but on server, we don't have it.\n",
    "            \n",
    "            # a receiver here, but on server, we don't have it.\n",
    "            # get the generator\n",
    "            rec_size = out.size(1)\n",
    "            if exit_flag != args.gated:\n",
    "                rec_ind = s_ind\n",
    "            rec_size2ind = channel2ind[rec_size]\n",
    "            # if we don't get all features, we need to use the generator\n",
    "\n",
    "            if args.generator:\n",
    "                if rec_size2ind != 3:\n",
    "                    cur_gen = generators[rec_size2ind]\n",
    "                    # load weights\n",
    "                    cur_gen.load_state_dict(torch.load('./Weights/cifar-10/generator_'+str(rec_size2ind)+'.pth'))\n",
    "                    cur_gen = cur_gen.cuda()\n",
    "                    out = cur_gen(out)\n",
    "\n",
    "            # skip the generator, create a tensor with all zeros\n",
    "            else:\n",
    "                if exit_flag != args.gated:\n",
    "                    n_out = torch.zeros(out.size(0), 32, 32, 32).cuda()\n",
    "                    n_out[:, rec_ind, :, :] = out\n",
    "                    out = n_out\n",
    "\n",
    "            # run the server model\n",
    "            out = server_model(out)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('enter')\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # we need the name of model, the name of dataset\n",
    "#     parser.add_argument('--dataset', type=str, default='cifar10', help='name of dataset')\n",
    "#     # parser.add_argument('--iot_model', type=str, default='mobilenetV2', help='name of the model on the iot')\n",
    "#     parser.add_argument('--reducer', type=str, default='entrophy', help='name of the reducer')\n",
    "#     parser.add_argument('--client', type=str, default='LTE', help='name of the network condition on the client side')\n",
    "#     parser.add_argument('--server', type=str, default='LTE', help='name of the network condition on the server side')\n",
    "#     parser.add_argument('--generator', type=str, default='None', help='name of the generator')\n",
    "#     # parser.add_argument('--server_model', type=str, default='mobilenetV2', help='name of the model on the server, should be the same as it on the iot')\n",
    "#     parser.add_argument('--device', type=str, default='home', help='run on which device, home, tintin, rpi, pico, jetson?')\n",
    "#     parser.add_argument('--model', type=str, default='mobilenetV2', help='name of the model')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n",
    "\n",
    "class custom_args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'cifar10'\n",
    "        self.model = 'mobilenetV2'\n",
    "        self.gated = True\n",
    "        self.ranker = 'entropy'\n",
    "        self.generator = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'dataset: '+self.dataset+', model: '+self.model+', gated: '+str(self.gated)+', ranker: '+self.ranker+', generator: '+str(self.generator)\n",
    "\n",
    "args = custom_args()\n",
    "print(args)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cifar10, model: mobilenetV2, gated: True, ranker: entropy, generator: True\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "The choosen gate is:  2\n",
      "Accuracy of the network on the 10000 test images: 13 %\n"
     ]
    }
   ],
   "source": [
    "# this is the total pipeline for the project\n",
    "# This file is for trainning\n",
    "# Run this on the server, or as we called offline. \n",
    "\n",
    "# with gated, no generator\n",
    "from Dataloaders.dataloader_cifar10 import Dataloader_cifar10\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from Models import mobilenetv2\n",
    "from Utils import utils\n",
    "from Models import gatedmodel\n",
    "from Models import generator\n",
    "\n",
    "def main(args):\n",
    "    # initial using mobilenetV2, and cifar10\n",
    "    # we need a if statement here to decide which model and dataset to use\n",
    "    # random_seed = 2024\n",
    "\n",
    "    # for training, it is for training the generator \n",
    "    # recall the graph, when we cut more features, the performance should be worse.\n",
    "    \n",
    "    # get the training loader, freeze the model. Where is the partitioning point? \n",
    "\n",
    "    # 1. get the train, test and val datasets, and labels.\n",
    "    if args.dataset == 'cifar10':\n",
    "        # return train, test, val, labels, these are all dataloaders\n",
    "        _, test, classes = Dataloader_cifar10(train_batch=128, test_batch=100, seed=2024)\n",
    "    elif args.dataset == 'cifar100':\n",
    "        pass\n",
    "    \n",
    "    # 2. transfer the dataset to fit the model, for the training, client and server model are all on the server\n",
    "    if args.model == 'mobilenetV2':\n",
    "        client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "    elif args.model == 'resnet':\n",
    "        pass\n",
    "\n",
    "    # 3. get the gating, gating here decides how many channels are transferred to the server\n",
    "    # simple version: a binary tree, complex version: model\n",
    "    # get a ranker to rank the channels, and get the top k channels\n",
    "    ranker = utils.ranker_entropy # input: embs, percentage, output: s_emb, s_ind\n",
    "\n",
    "    # 4. get the gated model, we have 3 models here\n",
    "    gated_rates = [0.25, 0.5, 0.75]\n",
    "    channel2ind = {8:0, 16:1, 24:2, 32:3}\n",
    "    input_channel = 32\n",
    "    gateds = []\n",
    "    for i in range(3):\n",
    "        gateds.append(gatedmodel.GatedRegression(\n",
    "            input_size=int(input_channel*gated_rates[i]),\n",
    "            weight=32,\n",
    "            height=32,\n",
    "            output_size=10)) #  input_size, weight, height, output_size=10\n",
    "\n",
    "    # 5. get the generator\n",
    "    generators = []\n",
    "    for i in range(3):\n",
    "        generators.append(generator.Generator(\n",
    "            inputsize=int(input_channel*gated_rates[i]), \n",
    "            hiddensize=32, \n",
    "            outputsize=32)) # inputsize, hiddensize, outputsize\n",
    "        \n",
    "    # 6. get the server \n",
    "    # server_model = some_model_function()\n",
    "    # server model is got above\n",
    "\n",
    "    # pipline data -> dataloader -> client_model -> gating -> reducer -> generator -> server_model\n",
    "    \n",
    "    # cuda may not have enough space for putting all the models\n",
    "    # client_model = client_model.cuda()\n",
    "    # server_model = server_model.cuda()\n",
    "    # for i in range(3):\n",
    "    #     gateds[i] = gateds[i].cuda()\n",
    "    #     generators[i] = generators[i].cuda()\n",
    "\n",
    "    # set them to eval\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    for i in range(3):\n",
    "        gateds[i].eval()\n",
    "        generators[i].eval()\n",
    "\n",
    "    globbal_threshold = 0.8 # set a small value first\n",
    "\n",
    "    # load the test data set the test\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        client_model = client_model.cuda()\n",
    "        server_model = server_model.cuda()\n",
    "        for data in test:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            # first, run the client model on the iot\n",
    "            out = client_model(images)\n",
    "            # second, run the ranker\n",
    "            exit_flag = args.gated\n",
    "            counter = 0\n",
    "            while exit_flag:\n",
    "                # get the emb and ind from the ranker\n",
    "                out = out.cpu()\n",
    "                s_emb, s_ind = ranker(out, gated_rates[counter])\n",
    "                s_emb = s_emb.cuda()\n",
    "                # give it to the gated model\n",
    "                cur_gated = gateds[counter]\n",
    "                # load the weights\n",
    "                cur_gated.load_state_dict(torch.load('./Weights/cifar-10/GatedRegression_'+str(counter)+'.pth'))\n",
    "                cur_gated = cur_gated.cuda()\n",
    "                g_emb = cur_gated(s_emb) # b, n\n",
    "                # get the argmax\n",
    "                g_conf = torch.max(g_emb, dim=1).values # b\n",
    "                # use the sigmoid to get the confidence\n",
    "                g_conf = torch.nn.functional.sigmoid(g_conf) # b\n",
    "                # if in a batch, get the average\n",
    "                g_conf = torch.mean(g_conf)\n",
    "                # print('The confidence is: ', g_conf.item())\n",
    "                if g_conf > globbal_threshold:\n",
    "                    exit_flag = not exit_flag\n",
    "                else:\n",
    "                    counter += 1\n",
    "\n",
    "            # check the exit flag and send\n",
    "            if exit_flag != args.gated:\n",
    "                print('The choosen gate is: ', counter)\n",
    "                out = s_emb\n",
    "\n",
    "            # a sender here, but on server, we don't have it.\n",
    "            \n",
    "            # a receiver here, but on server, we don't have it.\n",
    "            # get the generator\n",
    "            rec_size = out.size(1)\n",
    "            if exit_flag != args.gated:\n",
    "                rec_ind = s_ind\n",
    "            rec_size2ind = channel2ind[rec_size]\n",
    "            # if we don't get all features, we need to use the generator\n",
    "            if args.generator:\n",
    "                if rec_size2ind != 3:\n",
    "                    cur_gen = generators[rec_size2ind]\n",
    "                    # load weights\n",
    "                    cur_gen.load_state_dict(torch.load('./Weights/cifar-10/generator_'+str(rec_size2ind)+'.pth'))\n",
    "                    cur_gen = cur_gen.cuda()\n",
    "                    out = cur_gen(out)\n",
    "\n",
    "            # skip the generator, create a tensor with all zeros\n",
    "            else:\n",
    "                if exit_flag != args.gated:\n",
    "                    n_out = torch.zeros(out.size(0), 32, 32, 32).cuda()\n",
    "                    n_out[:, rec_ind, :, :] = out\n",
    "                    out = n_out\n",
    "\n",
    "            # run the server model\n",
    "            out = server_model(out)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('enter')\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # we need the name of model, the name of dataset\n",
    "#     parser.add_argument('--dataset', type=str, default='cifar10', help='name of dataset')\n",
    "#     # parser.add_argument('--iot_model', type=str, default='mobilenetV2', help='name of the model on the iot')\n",
    "#     parser.add_argument('--reducer', type=str, default='entrophy', help='name of the reducer')\n",
    "#     parser.add_argument('--client', type=str, default='LTE', help='name of the network condition on the client side')\n",
    "#     parser.add_argument('--server', type=str, default='LTE', help='name of the network condition on the server side')\n",
    "#     parser.add_argument('--generator', type=str, default='None', help='name of the generator')\n",
    "#     # parser.add_argument('--server_model', type=str, default='mobilenetV2', help='name of the model on the server, should be the same as it on the iot')\n",
    "#     parser.add_argument('--device', type=str, default='home', help='run on which device, home, tintin, rpi, pico, jetson?')\n",
    "#     parser.add_argument('--model', type=str, default='mobilenetV2', help='name of the model')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n",
    "\n",
    "class custom_args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'cifar10'\n",
    "        self.model = 'mobilenetV2'\n",
    "        self.gated = True\n",
    "        self.ranker = 'entropy'\n",
    "        self.generator = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'dataset: '+self.dataset+', model: '+self.model+', gated: '+str(self.gated)+', ranker: '+self.ranker+', generator: '+str(self.generator)\n",
    "\n",
    "args = custom_args()\n",
    "print(args)\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
