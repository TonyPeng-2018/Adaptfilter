{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/391] Loss_D: 1.4807 Loss_G: 0.8649 D(x): 0.4285 D(G(z)): 0.4526 / 0.4294\n",
      "[0/50][0/391] Loss_D: 1.1933 Loss_G: 0.9527 D(x): 0.5835 D(G(z)): 0.4669 / 0.3940\n",
      "[0/50][0/391] Loss_D: 0.9841 Loss_G: 1.1315 D(x): 0.6567 D(G(z)): 0.4196 / 0.3298\n",
      "[0/50][0/391] Loss_D: 0.8615 Loss_G: 1.2553 D(x): 0.6990 D(G(z)): 0.3841 / 0.2915\n",
      "[0/50][0/391] Loss_D: 0.7255 Loss_G: 1.4266 D(x): 0.7469 D(G(z)): 0.3392 / 0.2485\n",
      "[0/50][0/391] Loss_D: 0.6202 Loss_G: 1.5641 D(x): 0.7857 D(G(z)): 0.3039 / 0.2173\n",
      "[0/50][0/391] Loss_D: 0.5207 Loss_G: 1.7277 D(x): 0.8300 D(G(z)): 0.2766 / 0.1849\n",
      "[0/50][0/391] Loss_D: 0.4837 Loss_G: 1.8427 D(x): 0.8366 D(G(z)): 0.2545 / 0.1656\n",
      "[0/50][0/391] Loss_D: 0.4189 Loss_G: 2.0237 D(x): 0.8591 D(G(z)): 0.2239 / 0.1413\n",
      "[0/50][0/391] Loss_D: 0.3905 Loss_G: 2.1529 D(x): 0.8677 D(G(z)): 0.2097 / 0.1239\n",
      "[0/50][0/391] Loss_D: 0.3028 Loss_G: 2.4298 D(x): 0.9013 D(G(z)): 0.1751 / 0.0941\n",
      "[0/50][0/391] Loss_D: 0.3030 Loss_G: 2.4975 D(x): 0.8841 D(G(z)): 0.1551 / 0.0871\n",
      "[0/50][0/391] Loss_D: 0.2647 Loss_G: 2.6366 D(x): 0.9139 D(G(z)): 0.1559 / 0.0758\n",
      "[0/50][0/391] Loss_D: 0.3079 Loss_G: 2.7083 D(x): 0.8975 D(G(z)): 0.1652 / 0.0724\n",
      "[0/50][0/391] Loss_D: 0.2390 Loss_G: 2.9682 D(x): 0.9211 D(G(z)): 0.1414 / 0.0550\n",
      "[0/50][0/391] Loss_D: 0.2298 Loss_G: 3.0735 D(x): 0.9135 D(G(z)): 0.1245 / 0.0498\n",
      "[0/50][0/391] Loss_D: 0.2296 Loss_G: 3.1151 D(x): 0.9062 D(G(z)): 0.1178 / 0.0473\n",
      "[0/50][0/391] Loss_D: 0.2194 Loss_G: 3.2968 D(x): 0.9149 D(G(z)): 0.1173 / 0.0405\n",
      "[0/50][0/391] Loss_D: 0.1909 Loss_G: 3.5293 D(x): 0.9232 D(G(z)): 0.0987 / 0.0324\n",
      "[0/50][0/391] Loss_D: 0.1634 Loss_G: 3.8057 D(x): 0.9352 D(G(z)): 0.0876 / 0.0247\n",
      "[0/50][0/391] Loss_D: 0.1425 Loss_G: 4.0300 D(x): 0.9422 D(G(z)): 0.0755 / 0.0199\n",
      "[0/50][0/391] Loss_D: 0.1125 Loss_G: 4.2010 D(x): 0.9524 D(G(z)): 0.0597 / 0.0167\n",
      "[0/50][0/391] Loss_D: 0.1148 Loss_G: 4.2763 D(x): 0.9538 D(G(z)): 0.0629 / 0.0156\n",
      "[0/50][0/391] Loss_D: 0.1284 Loss_G: 4.3977 D(x): 0.9505 D(G(z)): 0.0711 / 0.0141\n",
      "[0/50][0/391] Loss_D: 0.1479 Loss_G: 4.1859 D(x): 0.9337 D(G(z)): 0.0714 / 0.0170\n",
      "[0/50][0/391] Loss_D: 0.1386 Loss_G: 4.5905 D(x): 0.9533 D(G(z)): 0.0821 / 0.0111\n",
      "[0/50][0/391] Loss_D: 0.1064 Loss_G: 4.7235 D(x): 0.9551 D(G(z)): 0.0569 / 0.0100\n",
      "[0/50][0/391] Loss_D: 0.1414 Loss_G: 3.8955 D(x): 0.9189 D(G(z)): 0.0438 / 0.0234\n",
      "[0/50][0/391] Loss_D: 0.1175 Loss_G: 5.4056 D(x): 0.9865 D(G(z)): 0.0973 / 0.0050\n",
      "[0/50][0/391] Loss_D: 0.0506 Loss_G: 5.5632 D(x): 0.9698 D(G(z)): 0.0178 / 0.0044\n",
      "[0/50][0/391] Loss_D: 0.0478 Loss_G: 4.9972 D(x): 0.9690 D(G(z)): 0.0149 / 0.0077\n",
      "[0/50][0/391] Loss_D: 0.0601 Loss_G: 4.6824 D(x): 0.9788 D(G(z)): 0.0338 / 0.0104\n",
      "[0/50][0/391] Loss_D: 0.0854 Loss_G: 5.6195 D(x): 0.9830 D(G(z)): 0.0627 / 0.0041\n",
      "[0/50][0/391] Loss_D: 0.0508 Loss_G: 5.5998 D(x): 0.9728 D(G(z)): 0.0215 / 0.0042\n",
      "[0/50][0/391] Loss_D: 0.0377 Loss_G: 5.3831 D(x): 0.9847 D(G(z)): 0.0218 / 0.0052\n",
      "[0/50][0/391] Loss_D: 0.0333 Loss_G: 5.3984 D(x): 0.9876 D(G(z)): 0.0203 / 0.0051\n",
      "[0/50][0/391] Loss_D: 0.0350 Loss_G: 5.3910 D(x): 0.9855 D(G(z)): 0.0200 / 0.0051\n",
      "[0/50][0/391] Loss_D: 0.0316 Loss_G: 5.5082 D(x): 0.9888 D(G(z)): 0.0199 / 0.0046\n",
      "[0/50][0/391] Loss_D: 0.0276 Loss_G: 5.6193 D(x): 0.9906 D(G(z)): 0.0178 / 0.0041\n",
      "[0/50][0/391] Loss_D: 0.0293 Loss_G: 5.6357 D(x): 0.9886 D(G(z)): 0.0174 / 0.0040\n",
      "[0/50][0/391] Loss_D: 0.0378 Loss_G: 5.6718 D(x): 0.9852 D(G(z)): 0.0221 / 0.0040\n",
      "[0/50][0/391] Loss_D: 0.0461 Loss_G: 6.1247 D(x): 0.9872 D(G(z)): 0.0318 / 0.0025\n",
      "[0/50][0/391] Loss_D: 0.0328 Loss_G: 6.2409 D(x): 0.9859 D(G(z)): 0.0181 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0337 Loss_G: 5.8493 D(x): 0.9819 D(G(z)): 0.0147 / 0.0033\n",
      "[0/50][0/391] Loss_D: 0.0367 Loss_G: 6.2342 D(x): 0.9917 D(G(z)): 0.0278 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0354 Loss_G: 6.1350 D(x): 0.9842 D(G(z)): 0.0188 / 0.0024\n",
      "[0/50][0/391] Loss_D: 0.0393 Loss_G: 6.1150 D(x): 0.9852 D(G(z)): 0.0238 / 0.0025\n",
      "[0/50][0/391] Loss_D: 0.0466 Loss_G: 5.8721 D(x): 0.9794 D(G(z)): 0.0218 / 0.0032\n",
      "[0/50][0/391] Loss_D: 0.0378 Loss_G: 6.4701 D(x): 0.9912 D(G(z)): 0.0283 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0241 Loss_G: 6.3384 D(x): 0.9868 D(G(z)): 0.0106 / 0.0021\n",
      "[0/50][0/391] Loss_D: 0.0175 Loss_G: 6.0949 D(x): 0.9923 D(G(z)): 0.0097 / 0.0025\n",
      "[0/50][0/391] Loss_D: 0.0257 Loss_G: 5.9892 D(x): 0.9898 D(G(z)): 0.0151 / 0.0030\n",
      "[0/50][0/391] Loss_D: 0.0231 Loss_G: 6.2767 D(x): 0.9935 D(G(z)): 0.0163 / 0.0021\n",
      "[0/50][0/391] Loss_D: 0.0223 Loss_G: 6.4336 D(x): 0.9931 D(G(z)): 0.0151 / 0.0019\n",
      "[0/50][0/391] Loss_D: 0.0257 Loss_G: 6.0992 D(x): 0.9859 D(G(z)): 0.0111 / 0.0027\n",
      "[0/50][0/391] Loss_D: 0.0231 Loss_G: 6.2648 D(x): 0.9939 D(G(z)): 0.0168 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0263 Loss_G: 6.1551 D(x): 0.9867 D(G(z)): 0.0126 / 0.0024\n",
      "[0/50][0/391] Loss_D: 0.0223 Loss_G: 6.5311 D(x): 0.9939 D(G(z)): 0.0159 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0199 Loss_G: 6.5290 D(x): 0.9906 D(G(z)): 0.0103 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0198 Loss_G: 6.3076 D(x): 0.9903 D(G(z)): 0.0097 / 0.0021\n",
      "[0/50][0/391] Loss_D: 0.0203 Loss_G: 6.5678 D(x): 0.9932 D(G(z)): 0.0133 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0176 Loss_G: 6.6442 D(x): 0.9920 D(G(z)): 0.0094 / 0.0015\n",
      "[0/50][0/391] Loss_D: 0.0128 Loss_G: 6.5946 D(x): 0.9941 D(G(z)): 0.0068 / 0.0016\n",
      "[0/50][0/391] Loss_D: 0.0175 Loss_G: 6.2746 D(x): 0.9907 D(G(z)): 0.0078 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0150 Loss_G: 6.5511 D(x): 0.9952 D(G(z)): 0.0101 / 0.0016\n",
      "[0/50][0/391] Loss_D: 0.0108 Loss_G: 6.7776 D(x): 0.9958 D(G(z)): 0.0065 / 0.0014\n",
      "[0/50][0/391] Loss_D: 0.0097 Loss_G: 6.6694 D(x): 0.9952 D(G(z)): 0.0049 / 0.0015\n",
      "[0/50][0/391] Loss_D: 0.0094 Loss_G: 6.5342 D(x): 0.9952 D(G(z)): 0.0046 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0094 Loss_G: 6.4644 D(x): 0.9965 D(G(z)): 0.0058 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0107 Loss_G: 6.5289 D(x): 0.9958 D(G(z)): 0.0064 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0112 Loss_G: 6.5378 D(x): 0.9949 D(G(z)): 0.0060 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0100 Loss_G: 6.4862 D(x): 0.9953 D(G(z)): 0.0052 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0100 Loss_G: 6.5858 D(x): 0.9965 D(G(z)): 0.0065 / 0.0016\n",
      "[0/50][0/391] Loss_D: 0.0096 Loss_G: 6.7509 D(x): 0.9968 D(G(z)): 0.0064 / 0.0014\n",
      "[0/50][0/391] Loss_D: 0.0089 Loss_G: 6.7887 D(x): 0.9971 D(G(z)): 0.0059 / 0.0013\n",
      "[0/50][0/391] Loss_D: 0.0079 Loss_G: 6.8545 D(x): 0.9978 D(G(z)): 0.0057 / 0.0012\n",
      "[0/50][0/391] Loss_D: 0.0106 Loss_G: 6.5120 D(x): 0.9936 D(G(z)): 0.0042 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0082 Loss_G: 6.4997 D(x): 0.9972 D(G(z)): 0.0054 / 0.0018\n",
      "[0/50][0/391] Loss_D: 0.0115 Loss_G: 6.2545 D(x): 0.9940 D(G(z)): 0.0052 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0106 Loss_G: 6.5338 D(x): 0.9972 D(G(z)): 0.0077 / 0.0017\n",
      "[0/50][0/391] Loss_D: 0.0082 Loss_G: 6.8421 D(x): 0.9980 D(G(z)): 0.0061 / 0.0012\n",
      "[0/50][0/391] Loss_D: 0.0142 Loss_G: 6.3792 D(x): 0.9913 D(G(z)): 0.0051 / 0.0020\n",
      "[0/50][0/391] Loss_D: 0.0101 Loss_G: 6.8863 D(x): 0.9987 D(G(z)): 0.0088 / 0.0012\n",
      "[0/50][0/391] Loss_D: 0.0087 Loss_G: 7.1153 D(x): 0.9973 D(G(z)): 0.0059 / 0.0009\n",
      "[0/50][0/391] Loss_D: 0.0112 Loss_G: 6.6468 D(x): 0.9930 D(G(z)): 0.0041 / 0.0016\n",
      "[0/50][0/391] Loss_D: 0.0091 Loss_G: 6.7521 D(x): 0.9973 D(G(z)): 0.0064 / 0.0014\n",
      "[0/50][0/391] Loss_D: 0.0088 Loss_G: 6.9851 D(x): 0.9969 D(G(z)): 0.0056 / 0.0011\n",
      "[0/50][0/391] Loss_D: 0.0063 Loss_G: 7.0874 D(x): 0.9976 D(G(z)): 0.0039 / 0.0010\n",
      "[0/50][0/391] Loss_D: 0.0069 Loss_G: 6.9049 D(x): 0.9966 D(G(z)): 0.0034 / 0.0011\n",
      "[0/50][0/391] Loss_D: 0.0086 Loss_G: 6.6790 D(x): 0.9954 D(G(z)): 0.0038 / 0.0014\n",
      "[0/50][0/391] Loss_D: 0.0084 Loss_G: 6.9036 D(x): 0.9970 D(G(z)): 0.0054 / 0.0012\n",
      "[0/50][0/391] Loss_D: 0.0060 Loss_G: 7.2452 D(x): 0.9978 D(G(z)): 0.0038 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0054 Loss_G: 7.2056 D(x): 0.9976 D(G(z)): 0.0030 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0055 Loss_G: 7.1444 D(x): 0.9974 D(G(z)): 0.0028 / 0.0009\n",
      "[0/50][0/391] Loss_D: 0.0064 Loss_G: 7.0678 D(x): 0.9970 D(G(z)): 0.0033 / 0.0010\n",
      "[0/50][0/391] Loss_D: 0.0055 Loss_G: 7.2625 D(x): 0.9980 D(G(z)): 0.0035 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0056 Loss_G: 7.2858 D(x): 0.9975 D(G(z)): 0.0030 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0070 Loss_G: 7.0745 D(x): 0.9962 D(G(z)): 0.0031 / 0.0010\n",
      "[0/50][0/391] Loss_D: 0.0046 Loss_G: 7.3212 D(x): 0.9988 D(G(z)): 0.0034 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0042 Loss_G: 7.4190 D(x): 0.9983 D(G(z)): 0.0025 / 0.0007\n",
      "[0/50][0/391] Loss_D: 0.0057 Loss_G: 7.1285 D(x): 0.9967 D(G(z)): 0.0024 / 0.0009\n",
      "[0/50][0/391] Loss_D: 0.0090 Loss_G: 6.7295 D(x): 0.9951 D(G(z)): 0.0039 / 0.0014\n",
      "[0/50][0/391] Loss_D: 0.0084 Loss_G: 7.7319 D(x): 0.9986 D(G(z)): 0.0070 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0068 Loss_G: 7.5349 D(x): 0.9959 D(G(z)): 0.0026 / 0.0006\n",
      "[0/50][0/391] Loss_D: 0.0070 Loss_G: 7.1229 D(x): 0.9964 D(G(z)): 0.0034 / 0.0009\n",
      "[0/50][0/391] Loss_D: 0.0076 Loss_G: 7.7122 D(x): 0.9987 D(G(z)): 0.0063 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0058 Loss_G: 7.8082 D(x): 0.9977 D(G(z)): 0.0035 / 0.0004\n",
      "[0/50][0/391] Loss_D: 0.0084 Loss_G: 7.1946 D(x): 0.9945 D(G(z)): 0.0028 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0098 Loss_G: 8.1656 D(x): 0.9978 D(G(z)): 0.0075 / 0.0003\n",
      "[0/50][0/391] Loss_D: 0.0067 Loss_G: 7.9771 D(x): 0.9957 D(G(z)): 0.0023 / 0.0004\n",
      "[0/50][0/391] Loss_D: 0.0058 Loss_G: 7.6801 D(x): 0.9974 D(G(z)): 0.0032 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0069 Loss_G: 8.3119 D(x): 0.9984 D(G(z)): 0.0053 / 0.0003\n",
      "[0/50][0/391] Loss_D: 0.0048 Loss_G: 8.3338 D(x): 0.9981 D(G(z)): 0.0029 / 0.0003\n",
      "[0/50][0/391] Loss_D: 0.0066 Loss_G: 7.7743 D(x): 0.9963 D(G(z)): 0.0028 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0093 Loss_G: 7.7819 D(x): 0.9954 D(G(z)): 0.0047 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0086 Loss_G: 8.3056 D(x): 0.9963 D(G(z)): 0.0048 / 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/391] Loss_D: 0.0084 Loss_G: 7.7406 D(x): 0.9946 D(G(z)): 0.0026 / 0.0005\n",
      "[0/50][0/391] Loss_D: 0.0073 Loss_G: 8.9324 D(x): 0.9987 D(G(z)): 0.0060 / 0.0002\n",
      "[0/50][0/391] Loss_D: 0.0037 Loss_G: 8.6816 D(x): 0.9979 D(G(z)): 0.0016 / 0.0002\n",
      "[0/50][0/391] Loss_D: 0.0030 Loss_G: 8.1120 D(x): 0.9984 D(G(z)): 0.0014 / 0.0004\n",
      "[0/50][0/391] Loss_D: 0.0064 Loss_G: 7.4413 D(x): 0.9969 D(G(z)): 0.0033 / 0.0008\n",
      "[0/50][0/391] Loss_D: 0.0078 Loss_G: 8.4221 D(x): 0.9985 D(G(z)): 0.0062 / 0.0003\n",
      "[0/50][0/391] Loss_D: 0.0184 Loss_G: 6.3701 D(x): 0.9886 D(G(z)): 0.0020 / 0.0022\n",
      "[0/50][0/391] Loss_D: 0.0224 Loss_G: 11.3032 D(x): 0.9990 D(G(z)): 0.0210 / 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 157\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# emb 1,b,c',h,w, inds 1,b,c' labels 1,b,c,h,w\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# squeeze the embeddings\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[48], line 118\u001b[0m, in \u001b[0;36mgenerator_dataloader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_folder\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgated\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/embeddings/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/iot/lib/python3.12/site-packages/torch/serialization.py:1381\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1381\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the generator used the part of features to make all data\n",
    "# We suppose that it is a GAN, DCGAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Models import mobilenetv2\n",
    "from Dataloaders import dataloader_cifar10\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "# client, and server model\n",
    "client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "\n",
    "# def the generator\n",
    "# https://github.com/pytorch/examples/blob/main/dcgan/main.py\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, outputsize):\n",
    "        super(Generator, self).__init__()\n",
    "        self.inputsize = inputsize # 8, 16, 24\n",
    "        self.outputsize = outputsize\n",
    "        self.hiddensize = hiddensize\n",
    "        self.section1 = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(self.inputsize, hiddensize * 8, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.section2 = nn.Sequential(\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(hiddensize * 8, hiddensize * 4, 3, 1, padding=1 , bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(hiddensize * 4, hiddensize * 2, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 2),\n",
    "            nn.ReLU(True),\n",
    "            # # state size. (ngf*2) x 16 x 16\n",
    "            # nn.ConvTranspose2d(hiddensize * 2, hiddensize, 4, 1, 0, bias=False),\n",
    "            # nn.BatchNorm2d(hiddensize),\n",
    "            # nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(hiddensize * 2, self.outputsize, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.section1(input)\n",
    "        output = self.section2(output)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, outputsize, positionsize):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.inputsize = inputsize # 8, 16, 24\n",
    "        self.outputsize = outputsize\n",
    "        self.hiddensize = hiddensize\n",
    "        self.positionsize = positionsize\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(inputsize, hiddensize, 3, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(hiddensize, hiddensize*2, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(hiddensize * 2, hiddensize * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(hiddensize * 4, hiddensize * 8, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # # state size. (ndf*8) x 4 x 4\n",
    "            # nn.Conv2d(hiddensize * 8, inputsize, 3, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(hiddensize * 8),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.position_encoder = nn.Sequential(\n",
    "            nn.Linear(positionsize, 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(32+4096, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input, input2):\n",
    "        out1 = self.main(input) # hiddensize * n, ? ,32 ,32\n",
    "        out2 = self.position_encoder(input2) # 32 -> 32\n",
    "        # flatten and concatenate the two features \n",
    "        out1 = out1.flatten(start_dim=1)\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        output = self.output_layer(out)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "# dataloader using the embeddings\n",
    "class generator_dataloader():\n",
    "    def __init__(self, embeddings_folder, labels_folder, gated):\n",
    "        self.embeddings_folder = embeddings_folder\n",
    "        self.gated = gated\n",
    "        self.embeddings_files = sorted(os.listdir(embeddings_folder+str(gated)+'/embeddings/'))\n",
    "        self.labels_folder = labels_folder\n",
    "        self.labels_files = sorted(os.listdir(labels_folder+'embeddings/'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings_files.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.embeddings = torch.load(self.embeddings_folder+str(self.gated)+'/embeddings/' + self.embeddings_files[idx])\n",
    "        self.inds = self.embeddings[1]\n",
    "        self.embeddings = self.embeddings[0]\n",
    "        self.labels = torch.load(self.labels_folder+'embeddings/' + self.labels_files[idx])\n",
    "        return self.embeddings, self.inds, self.labels\n",
    "\n",
    "# dataloader using the embeddings\n",
    "train_dataloaders = []\n",
    "for i in range(3):\n",
    "    train_dataloaders.append(generator_dataloader(embeddings_folder='../data/cifar-10-embedding-entropy/', labels_folder='../data/cifar-10-embedding-3/', gated=i))\n",
    "\n",
    "epochs = 50\n",
    "# we have 3 generators for 3 discriminators\n",
    "Generators = []\n",
    "inputsizes = [8, 16, 24]\n",
    "for i in range(3):\n",
    "    Generators.append(Generator(inputsize=inputsizes[i], hiddensize=32, outputsize=32).cuda())\n",
    "Discriminators = []\n",
    "for i in range(3):\n",
    "    Discriminators.append(Discriminator(inputsize=32, hiddensize=32, outputsize=1, positionsize=32).cuda())\n",
    "\n",
    "# make the optimizers\n",
    "optimizers_G = []\n",
    "optimizers_D = []\n",
    "for i in range(3):\n",
    "    optimizers_G.append(optim.Adam(Generators[i].parameters(), lr=0.0002, betas=(0.5, 0.999)))\n",
    "    optimizers_D.append(optim.Adam(Discriminators[i].parameters(), lr=0.0002, betas=(0.5, 0.999)))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noises = []\n",
    "batch_size = 128\n",
    "for i in range(3):\n",
    "    fixed_noises.append(torch.randn(batch_size, inputsizes[i], 1, 1).cuda())\n",
    "real_flag = 1\n",
    "fake_flag = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range (3):\n",
    "        for j, data in enumerate(train_dataloaders[i]):\n",
    "            emb, ind, label = data # emb 1,b,c',h,w, inds 1,b,c' labels 1,b,c,h,w\n",
    "            # squeeze the embeddings\n",
    "            emb, label = emb.squeeze(0), label.squeeze(0) # b,c',h,w\n",
    "            # get the embeddings\n",
    "            emb = emb.cuda()\n",
    "            label = label.cuda()\n",
    "            # create a n*c one hot vector\n",
    "            one_hot = torch.zeros(emb.shape[0], 32).cuda()\n",
    "            one_hot[ind] = 1\n",
    "            # ind = ind.cuda() # it is not used here. How to do the positional encoding?\n",
    "            # train the discriminator\n",
    "            # train with real\n",
    "\n",
    "            netG = Generators[i].cuda()\n",
    "            netD = Discriminators[i].cuda()\n",
    "            optG = optimizers_G[i]\n",
    "            optD = optimizers_D[i]\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = label\n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), real_flag, device='cuda', dtype=real_cpu.dtype)\n",
    "            output = netD(real_cpu, one_hot)\n",
    "            errD_real = criterion(output, real_label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(b_size, inputsizes[i], 32, 32).cuda()\n",
    "            fake = netG(noise)\n",
    "            # print(fake.shape)\n",
    "            fake_label = torch.full((b_size,), fake_flag, device='cuda', dtype=real_cpu.dtype)\n",
    "            # add fake_positional encoding\n",
    "            fake_ones = torch.zeros(b_size, 32).cuda()\n",
    "            # add the fake ones\n",
    "            fake_ind = torch.randint(0, 32, (inputsizes[i], 1)).cuda()\n",
    "            fake_ones[fake_ind] = 1\n",
    "            output = netD(fake.detach(), fake_ones)\n",
    "            errD_fake = criterion(output, fake_label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optD.step()\n",
    "\n",
    "            # update Generator\n",
    "            netG.zero_grad()\n",
    "            fake_label.fill_(real_flag)\n",
    "            output = netD(fake, fake_ones)\n",
    "            errG = criterion(output, fake_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optG.step()\n",
    "\n",
    "        # print error\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, epochs, i, len(train_dataloaders[i]), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # save the model\n",
    "        torch.save(netG.state_dict(), './Weights/cifar-10/generator_new_'+str(i)+'.pth')\n",
    "        torch.save(netD.state_dict(), './Weights/cifar-10/discriminator_new_'+str(i)+'.pth')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the generator used the part of features to make all data\n",
    "# We suppose that it is a GAN, DCGAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Models import mobilenetv2\n",
    "from Dataloaders import dataloader_cifar10\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "# client, and server model\n",
    "client_model, server_model = mobilenetv2.stupid_model_splitter(weight_path='./Weights/cifar-10/MobileNetV2.pth')\n",
    "\n",
    "# def the generator\n",
    "# https://github.com/pytorch/examples/blob/main/dcgan/main.py\n",
    "class GeneratorV2(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, outputsize):\n",
    "        super(Generator, self).__init__()\n",
    "        self.inputsize = inputsize # 8, 16, 24\n",
    "        self.outputsize = outputsize\n",
    "        self.hiddensize = hiddensize\n",
    "        self.section1 = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(self.inputsize, hiddensize * 8, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.section2 = nn.Sequential(\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(hiddensize * 8, hiddensize * 4, 3, 1, padding=1 , bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(hiddensize * 4, hiddensize * 2, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.BatchNorm2d(hiddensize * 2),\n",
    "            nn.ReLU(True),\n",
    "            # # state size. (ngf*2) x 16 x 16\n",
    "            # nn.ConvTranspose2d(hiddensize * 2, hiddensize, 4, 1, 0, bias=False),\n",
    "            # nn.BatchNorm2d(hiddensize),\n",
    "            # nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(hiddensize * 2, self.outputsize, 3, 1, padding=1, bias=False, dilation=1),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.section1(input)\n",
    "        output = self.section2(output)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, outputsize, positionsize):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.inputsize = inputsize # 8, 16, 24\n",
    "        self.outputsize = outputsize\n",
    "        self.hiddensize = hiddensize\n",
    "        self.positionsize = positionsize\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(inputsize, hiddensize, 3, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(hiddensize, hiddensize*2, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(hiddensize * 2, hiddensize * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(hiddensize * 4, hiddensize * 8, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hiddensize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # # state size. (ndf*8) x 4 x 4\n",
    "            # nn.Conv2d(hiddensize * 8, inputsize, 3, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(hiddensize * 8),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.position_encoder = nn.Sequential(\n",
    "            nn.Linear(positionsize, 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(32+4096, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input, input2):\n",
    "        out1 = self.main(input) # hiddensize * n, ? ,32 ,32\n",
    "        out2 = self.position_encoder(input2) # 32 -> 32\n",
    "        # flatten and concatenate the two features \n",
    "        out1 = out1.flatten(start_dim=1)\n",
    "        out = torch.cat((out1, out2), 1)\n",
    "        output = self.output_layer(out)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "# dataloader using the embeddings\n",
    "class generator_dataloader():\n",
    "    def __init__(self, embeddings_folder, labels_folder, gated):\n",
    "        self.embeddings_folder = embeddings_folder\n",
    "        self.gated = gated\n",
    "        self.embeddings_files = sorted(os.listdir(embeddings_folder+str(gated)+'/embeddings/'))\n",
    "        self.labels_folder = labels_folder\n",
    "        self.labels_files = sorted(os.listdir(labels_folder+'embeddings/'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings_files.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.embeddings = torch.load(self.embeddings_folder+str(self.gated)+'/embeddings/' + self.embeddings_files[idx])\n",
    "        self.inds = self.embeddings[1]\n",
    "        self.embeddings = self.embeddings[0]\n",
    "        self.labels = torch.load(self.labels_folder+'embeddings/' + self.labels_files[idx])\n",
    "        return self.embeddings, self.inds, self.labels\n",
    "\n",
    "# dataloader using the embeddings\n",
    "train_dataloaders = []\n",
    "for i in range(3):\n",
    "    train_dataloaders.append(generator_dataloader(embeddings_folder='../data/cifar-10-embedding-entropy/', labels_folder='../data/cifar-10-embedding-3/', gated=i))\n",
    "\n",
    "epochs = 50\n",
    "# we have 3 generators for 3 discriminators\n",
    "Generators = []\n",
    "inputsizes = [8, 16, 24]\n",
    "for i in range(3):\n",
    "    Generators.append(Generator(inputsize=inputsizes[i], hiddensize=32, outputsize=32).cuda())\n",
    "Discriminators = []\n",
    "for i in range(3):\n",
    "    Discriminators.append(Discriminator(inputsize=32, hiddensize=32, outputsize=1, positionsize=32).cuda())\n",
    "\n",
    "# make the optimizers\n",
    "optimizers_G = []\n",
    "optimizers_D = []\n",
    "for i in range(3):\n",
    "    optimizers_G.append(optim.Adam(Generators[i].parameters(), lr=0.0002, betas=(0.5, 0.999)))\n",
    "    optimizers_D.append(optim.Adam(Discriminators[i].parameters(), lr=0.0002, betas=(0.5, 0.999)))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noises = []\n",
    "batch_size = 128\n",
    "for i in range(3):\n",
    "    fixed_noises.append(torch.randn(batch_size, inputsizes[i], 1, 1).cuda())\n",
    "real_flag = 1\n",
    "fake_flag = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range (3):\n",
    "        for j, data in enumerate(train_dataloaders[i]):\n",
    "            emb, ind, label = data # emb 1,b,c',h,w, inds 1,b,c' labels 1,b,c,h,w\n",
    "            # squeeze the embeddings\n",
    "            emb, label = emb.squeeze(0), label.squeeze(0) # b,c',h,w\n",
    "            # get the embeddings\n",
    "            emb = emb.cuda()\n",
    "            label = label.cuda()\n",
    "            # create a n*c one hot vector\n",
    "            one_hot = torch.zeros(emb.shape[0], 32).cuda()\n",
    "            one_hot[ind] = 1\n",
    "            # ind = ind.cuda() # it is not used here. How to do the positional encoding?\n",
    "            # train the discriminator\n",
    "            # train with real\n",
    "\n",
    "            netG = Generators[i].cuda()\n",
    "            netD = Discriminators[i].cuda()\n",
    "            optG = optimizers_G[i]\n",
    "            optD = optimizers_D[i]\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_cpu = label\n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), real_flag, device='cuda', dtype=real_cpu.dtype)\n",
    "            output = netD(real_cpu, one_hot)\n",
    "            errD_real = criterion(output, real_label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(b_size, inputsizes[i], 32, 32).cuda()\n",
    "            fake = netG(noise)\n",
    "            # print(fake.shape)\n",
    "            fake_label = torch.full((b_size,), fake_flag, device='cuda', dtype=real_cpu.dtype)\n",
    "            # add fake_positional encoding\n",
    "            fake_ones = torch.zeros(b_size, 32).cuda()\n",
    "            # add the fake ones\n",
    "            fake_ind = torch.randint(0, 32, (inputsizes[i], 1)).cuda()\n",
    "            fake_ones[fake_ind] = 1\n",
    "            output = netD(fake.detach(), fake_ones)\n",
    "            errD_fake = criterion(output, fake_label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optD.step()\n",
    "\n",
    "            # update Generator\n",
    "            netG.zero_grad()\n",
    "            fake_label.fill_(real_flag)\n",
    "            output = netD(fake, fake_ones)\n",
    "            errG = criterion(output, fake_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optG.step()\n",
    "\n",
    "        # print error\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, epochs, i, len(train_dataloaders[i]), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # save the model\n",
    "        torch.save(netG.state_dict(), './Weights/cifar-10/generator_new_'+str(i)+'.pth')\n",
    "        torch.save(netD.state_dict(), './Weights/cifar-10/discriminator_new_'+str(i)+'.pth')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
