{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Models import mobilenetv2, resnet\n",
    "from Dataloaders import dataloader_cifar10, dataloader_image_20\n",
    "from Models import gatedmodel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "models_name = {'mobile':mobilenetv2, 'resnet':resnet}\n",
    "datasets_name = {'cifar':dataloader_cifar10, 'imagenet':dataloader_image_20}\n",
    "middle_size_mobile = {x:2**x for x in range(5)}\n",
    "middle_size_resnet = {x:2**x for x in range(6)}\n",
    "middles_size = {'mobile':middle_size_mobile, 'resnet':middle_size_resnet}\n",
    "\n",
    "quantization = False\n",
    "int_trans = False\n",
    "device = 'cuda:0'\n",
    "\n",
    "def float_to_uint(x):\n",
    "    return torch.round(x * 255).int()\n",
    "\n",
    "def uint_to_float(x):\n",
    "    return x.float() / 255\n",
    "\n",
    "def MSE(x, y):\n",
    "    return torch.mean((x - y)**2)\n",
    "\n",
    "def KL_div(x, y):\n",
    "    return torch.mean(x * torch.log(x/y))\n",
    "\n",
    "def re_transform_cifar(x):\n",
    "    x = x * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def re_transform_imagenet(x):\n",
    "    x = x * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def transform_cifar(x):\n",
    "    x = (x - torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)) / torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)\n",
    "    return x\n",
    "\n",
    "def transform_imagenet(x):\n",
    "    x = (x - torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [00:02<03:20, 49.35it/s]\n",
      "  1%|          | 100/10000 [00:03<05:36, 29.44it/s]\n"
     ]
    }
   ],
   "source": [
    "width = 16\n",
    "height = 16\n",
    "client, server = mobilenetv2.mobilenetv2_splitter(num_classes=10, weight_root='./Weights/cifar-10')\n",
    "\n",
    "_, test, _ = dataloader_cifar10.Dataloader_cifar10_val(datasetpath='../data/', test_batch=1)\n",
    "_, test2, _ = dataloader_cifar10.Dataloader_cifar10_val(datasetpath='../data/', test_batch=1, normalize=False)\n",
    "\n",
    "middle_models = []\n",
    "for i in range (len(middle_size_mobile)):\n",
    "    middle = mobilenetv2.MobileNetV2_middle(middle=middle_size_mobile[i])\n",
    "    middle.load_state_dict(torch.load('mobile_cifar-10_middle_%s.pth'%str(middle_size_mobile[i])))\n",
    "    middle_models.append(middle)\n",
    "\n",
    "# eval and cuda\n",
    "client = client.eval().to(device)\n",
    "server = server.eval().to(device)\n",
    "for middle in middle_models:\n",
    "    middle = middle.eval().to(device)\n",
    "\n",
    "# use the quantization\n",
    "client = torch.ao.quantization.quantize_dynamic(client, dtype=torch.qint8)\n",
    "for middle in middle_models:\n",
    "    middle.in_layer = torch.ao.quantization.quantize_dynamic(middle.in_layer, dtype=torch.qint8)\n",
    "\n",
    "jpeg_list = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "middle_results = np.zeros((len(middle_models), len(test.dataset)))\n",
    "jpeg_results = np.zeros((len(jpeg_list), len(test.dataset)))\n",
    "\n",
    "stop_ind = 100\n",
    "with torch.no_grad():\n",
    "    # get the conf for each middles\n",
    "    correct_middle = np.zeros(len(middle_models))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "\n",
    "        original_div = None\n",
    "        data, target = data.to(device).detach(), target.to(device).detach()\n",
    "        out = client(data).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "\n",
    "        for j in range(len(middle_models)):\n",
    "            middle = middle_models[j]\n",
    "            out = client(data).detach()\n",
    "            out = middle.in_layer(out)\n",
    "            out = float_to_uint(out)\n",
    "            out = uint_to_float(out)\n",
    "            out = middle.out_layer(out)\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            middle_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_middle[j] += 1\n",
    "\n",
    "    correct_jpeg = np.zeros(len(jpeg_list))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test2)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "        data, target = data, target.to(device).detach()\n",
    "        out = transform_cifar(data)\n",
    "        data = data.to(device).detach()\n",
    "        out = out.to(device)\n",
    "        out = client(out).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "        for j in range(len(jpeg_list)):\n",
    "            jpeg = jpeg_list[j]\n",
    "            out = data.squeeze(0).cpu()\n",
    "            out = out.permute(1, 2, 0).numpy()\n",
    "            out = out * 255\n",
    "            out = out.astype(np.uint8)\n",
    "            # transform gbr to rgb\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "            # store out\n",
    "            # cv2.imwrite('temp.jpg', out)\n",
    "            out = cv2.imencode('.jpg', out, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])[1]\n",
    "            out = cv2.imdecode(out, cv2.IMREAD_COLOR)\n",
    "            # transfor rgb to gbr\n",
    "            # cv2.imwrite('temp%d.jpg'%j, out)\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n",
    "            out = out.astype(np.float16) / 255\n",
    "            out = torch.tensor(out).permute(2, 0, 1).unsqueeze(0)\n",
    "            out = transform_cifar(out).to(device)\n",
    "            out = client(out).detach()\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            # print if the original_div == out max\n",
    "            # print(torch.argmax(original_div) == torch.argmax(out))\n",
    "            jpeg_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_jpeg[j] += 1\n",
    "\n",
    "middle_pd = pd.DataFrame(middle_results.T, columns=['%d%%'%(middle_size_mobile[x]*100//32) for x in range(len(middle_size_mobile))])\n",
    "jpeg_pd = pd.DataFrame(jpeg_results.T, columns=[jpeg_list[x] for x in range(len(jpeg_list))])\n",
    "correct_jpeg = 1 - correct_jpeg / stop_ind\n",
    "correct_middle = 1 - correct_middle / stop_ind \n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=middle_pd, palette='Set3')\n",
    "plt.xlabel('Representation Percentage', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['3%', '6%', '12%', '25%', '50%']\n",
    "ax2.plot(temp_x, correct_middle, color='r', label='AVG MR', linewidth=2)\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.legend()\n",
    "plt.savefig('middle_boxplot_cifar.pdf', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=jpeg_pd, palette='Set3')\n",
    "plt.xlabel('JPEG Quality', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['10', '20', '30', '40', '50', '60', '70', '80', '90']\n",
    "ax2.plot(temp_x, correct_jpeg, label='JPEG')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.savefig('jpeg_boxplot_cifar.pdf', bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 100/5087 [00:02<02:19, 35.81it/s]\n",
      "  1%|          | 26/5087 [00:03<12:28,  6.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(out)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_cifar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m out \u001b[38;5;241m=\u001b[39m client(out)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     90\u001b[0m out \u001b[38;5;241m=\u001b[39m server(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "width = 16\n",
    "height = 16\n",
    "client, server = resnet.resnet_splitter(num_classes=1000, weight_root='./Weights/imagenet', layers=50)\n",
    "\n",
    "_, test, _ = dataloader_image_20.Dataloader_imagenet_20_integrated(test_batch=1)\n",
    "_, test2, _ = dataloader_image_20.Dataloader_imagenet_20_integrated(test_batch=1, transform=False)\n",
    "\n",
    "middle_models = []\n",
    "for i in range (len(middle_size_resnet)):\n",
    "    middle = resnet.resnet_middle(middle=middle_size_resnet[i])\n",
    "    middle.load_state_dict(torch.load('resnet_imagenet_middle_%s.pth'%str(middle_size_resnet[i])))\n",
    "    middle_models.append(middle)\n",
    "\n",
    "# eval and cuda\n",
    "client = client.eval().to(device)\n",
    "server = server.eval().to(device)\n",
    "for middle in middle_models:\n",
    "    middle = middle.eval().to(device)\n",
    "\n",
    "# use the quantization\n",
    "client = torch.ao.quantization.quantize_dynamic(client, dtype=torch.qint8)\n",
    "for middle in middle_models:\n",
    "    middle.in_layer = torch.ao.quantization.quantize_dynamic(middle.in_layer, dtype=torch.qint8)\n",
    "\n",
    "jpeg_list = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "middle_results = np.zeros((len(middle_models), len(test.dataset)))\n",
    "jpeg_results = np.zeros((len(jpeg_list), len(test.dataset)))\n",
    "\n",
    "stop_ind = 100\n",
    "with torch.no_grad():\n",
    "    # get the conf for each middles\n",
    "    correct_middle = np.zeros(len(middle_models))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "\n",
    "        original_div = None\n",
    "        data, target = data.to(device).detach(), target.to(device).detach()\n",
    "        out = client(data).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "\n",
    "        for j in range(len(middle_models)):\n",
    "            middle = middle_models[j]\n",
    "            out = client(data).detach()\n",
    "            out = middle.in_layer(out)\n",
    "            out = float_to_uint(out)\n",
    "            out = uint_to_float(out)\n",
    "            out = middle.out_layer(out)\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            middle_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_middle[j] += 1\n",
    "\n",
    "    correct_jpeg = np.zeros(len(jpeg_list))\n",
    "\n",
    "    for i, (data, target) in enumerate(tqdm(test2)):\n",
    "        if i == stop_ind:\n",
    "            break\n",
    "        data, target = data, target.to(device).detach()\n",
    "        out = transform_cifar(data)\n",
    "        data = data.to(device).detach()\n",
    "        out = out.to(device)\n",
    "        out = client(out).detach()\n",
    "        out = server(out)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "        original_div = out.clone().detach()\n",
    "        for j in range(len(jpeg_list)):\n",
    "            jpeg = jpeg_list[j]\n",
    "            out = data.squeeze(0).cpu()\n",
    "            out = out.permute(1, 2, 0).numpy()\n",
    "            out = out * 255\n",
    "            out = out.astype(np.uint8)\n",
    "            # transform gbr to rgb\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "            # store out\n",
    "            # cv2.imwrite('temp.jpg', out)\n",
    "            out = cv2.imencode('.jpg', out, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])[1]\n",
    "            out = cv2.imdecode(out, cv2.IMREAD_COLOR)\n",
    "            # transfor rgb to gbr\n",
    "            # cv2.imwrite('temp%d.jpg'%j, out)\n",
    "            out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n",
    "            out = out.astype(np.float16) / 255\n",
    "            out = torch.tensor(out).permute(2, 0, 1).unsqueeze(0)\n",
    "            out = transform_cifar(out).to(device)\n",
    "            out = client(out).detach()\n",
    "            out = server(out)\n",
    "            out = torch.nn.functional.softmax(out, dim=1)\n",
    "            # print if the original_div == out max\n",
    "            # print(torch.argmax(original_div) == torch.argmax(out))\n",
    "            jpeg_results[j, i] = KL_div(out, original_div)\n",
    "            if torch.argmax(out) == target:\n",
    "                correct_jpeg[j] += 1\n",
    "\n",
    "middle_pd = pd.DataFrame(middle_results.T, columns=['%d%%'%(middle_size_resnet[x]*100//64) for x in range(len(middle_size_resnet))])\n",
    "jpeg_pd = pd.DataFrame(jpeg_results.T, columns=[jpeg_list[x] for x in range(len(jpeg_list))])\n",
    "correct_jpeg = 1 - correct_jpeg / stop_ind\n",
    "correct_middle = 1 - correct_middle / stop_ind \n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=middle_pd, palette='Set3', legend='samples')\n",
    "plt.xlabel('Representation Percentage', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "# sns.set_theme(style=\"white\", palette=None )\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['1%', '3%', '6%', '12%', '25%', '50%']\n",
    "ax2.plot(temp_x, correct_middle, label='Middle', width=0.5, legend='AVG MR')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.legend()\n",
    "plt.savefig('middle_boxplot_imagenet.pdf', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.boxplot(data=jpeg_pd, palette='Set3')\n",
    "plt.xlabel('JPEG Quality', fontsize=12)\n",
    "plt.ylabel('KL Divergence', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax2 = plt.twinx()\n",
    "temp_x = ['10', '20', '30', '40', '50', '60', '70', '80', '90']\n",
    "ax2.plot(temp_x, correct_jpeg, label='JPEG')\n",
    "ax2.set_ylabel('Misclassification Rate', fontsize=12)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(False)\n",
    "plt.savefig('jpeg_boxplot_imagenet.pdf', bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
